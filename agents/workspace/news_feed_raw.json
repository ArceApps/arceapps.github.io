[
  {
    "source": "Android Developers Blog",
    "title": "Media3 1.9.0 - What’s new",
    "link": "http://android-developers.googleblog.com/2025/12/media3-190-whats-new.html",
    "pubDate": "2025-12-19T22:00:00.000Z",
    "contentSnippet": "Posted by Kristina Simakova, Engineering Manager\n\n\n\n\n\n\nMedia3 1.9.0 – What's new?\n\nMedia3 1.9.0 is out! Besides the usual bug fixes and performance improvements, the latest release also contains four new or largely rewritten modules:\n\nmedia3-inspector - Extract metadata and frames outside of playback\n\nmedia3-ui-compose-material3 - Build a basic Material3 Compose Media UI in just a few steps\n\nmedia3-cast - Automatically handle transitions between Cast and local playbacks\n\nmedia3-decoder-av1 - Consistent AV1 playback with the rewritten extension decoder based on the dav1d library\n\n\nWe also added caching and memory management improvements to PreloadManager, and provided several new ExoPlayer, Transformer and MediaSession simplifications. \n\nThis release also gives you the first experimental access to CompositionPlayer to preview media edits.  \n\nRead on to find out more, and as always please check out the full release notes for a comprehensive overview of changes in this release.\n\nExtract metadata and frames outside of playback\nThere are many cases where you want to inspect media without starting a playback. For example, you might want to detect which formats it contains or what its duration is, or to retrieve thumbnails.\nThe new media3-inspector module combines all utilities to inspect media without playback in one place:\n\nMetadataRetriever to read duration, format and static metadata from a MediaItem.\n\nFrameExtractor to get frames or thumbnails from an item. \n\nMediaExtractorCompat as a direct replacement for the Android platform MediaExtractor class, to get detailed information about samples in the file.\n\n\nMetadataRetriever and FrameExtractor follow a simple AutoCloseable pattern. Have a look at our new guide pages for more details.\n\nsuspend fun extractThumbnail(mediaItem: MediaItem) {\n  FrameExtractor.Builder(context, mediaItem).build().use {\n    val thumbnail = frameExtractor.getThumbnail().await()\n  } \n}\n\nBuild a basic Material3 Compose Media UI in just a few steps\nIn previous releases we started providing connector code between Compose UI elements and your Player instance. With Media3 1.9.0, we added a new module media3-ui-compose-material3 with fully-styled Material3 buttons and content elements. They allow you to build a media UI in just a few steps, while providing all the flexibility to customize style. If you prefer to build your own UI style, you can use the building blocks that take care of all the update and connection logic, so you only need to concentrate on designing the UI element. Please check out our extended guide pages for the Compose UI modules.\n\n\nWe are also still working on even more Compose components, like a prebuilt seek bar, a complete out-of-the-box replacement for PlayerView, as well as subtitle and ad integration.\n\n@Composable\nfun SimplePlayerUI(player: Player, modifier: Modifier = Modifier) {\n  Column(modifier) {\n    ContentFrame(player)  // Video surface and shutter logic\n    Row (Modifier.align(Alignment.CenterHorizontally)) {                 \n      SeekBackButton(player)   // Simple controls\n      PlayPauseButton(player)\n      SeekForwardButton(player)\n    }\n  }\n}\n\n\n\nSimple Compose player UI with out-of-the-box elements\n\nAutomatically handle transitions between Cast and local playbacks\nThe CastPlayer in the media3-cast module has been rewritten to automatically handle transitions between local playback (for example with ExoPlayer) and remote Cast playback.\n\n\nWhen you set up your MediaSession, simply build a CastPlayer around your ExoPlayer and add a MediaRouteButton to your UI and you're done!\n\n// MediaSession setup with CastPlayer \nval exoPlayer = ExoPlayer.Builder(context).build()\nval castPlayer = CastPlayer.Builder(context).setLocalPlayer(exoPlayer).build()\nval session = MediaSession.Builder(context, castPlayer).build()\n// MediaRouteButton in UI \n@Composable fun UIWithMediaRouteButton() {\n  MediaRouteButton()\n}\n\n\n\n \nNew CastPlayer integration in Media3 session demo app\n\nConsistent AV1 playback with the rewritten extension based on dav1d\nThe 1.9.0 release contains a completely rewritten AV1 extension module based on the popular dav1d library. \nAs with all extension decoder modules, please note that it requires building from source to bundle the relevant native code correctly. Bundling a decoder provides consistency and format support across all devices, but because it runs the decoding in your process, it's best suited for content you can trust. \n\nIntegrate caching and memory management into PreloadManager\nWe made our PreloadManager even better as well. It already enabled you to preload media into memory outside of playback and then seamlessly hand it over to a player when needed. Although pretty performant, you still had to be careful to not exceed memory limits by accidentally preloading too much. So with Media3 1.9.0, we added two features that makes this a lot easier and more stable:\n\n\n\n\nCaching support – When defining how far to preload, you can now choose PreloadStatus.specifiedRangeCached(0, 5000) as a target state for preloaded items. This will add the specified range to your cache on disk instead of loading the data to memory. With this, you can provide a much larger range of items for preloading as the ones further away from the current item no longer need to occupy memory. Note that this requires setting a Cache in DefaultPreloadManager.Builder.\n\nAutomatic memory management – We also updated our LoadControl interface to better handle the preload case so you are now able to set an explicit upper memory limit for all preloaded items in memory. It's 144 MB by default, and you can configure the limit in DefaultLoadControl.Builder. The DefaultPreloadManager will automatically stop preloading once the limit is reached, and automatically releases memory of lower priority items if required.\n\n\nRely on new simplified default behaviors in ExoPlayer\nAs always, we added lots of incremental improvements to ExoPlayer as well. To name just a few:\n\nMute and unmute – We already had a setVolume method, but have now added the convenience mute and unmute methods to easily restore the previous volume without keeping track of it yourself.\n\nStuck player detection – In some rare cases the player can get stuck in a buffering or playing state without making any progress, for example, due to codec issues or misconfigurations. Your users will be annoyed, but you never see these issues in your analytics! To make this more obvious, the player now reports a StuckPlayerException when it detects a stuck state.\n\nWakelock by default – The wake lock management was previously opt-in, resulting in hard to find edge cases where playback progress can be delayed a lot when running in the background. Now this feature is opt-out, so you don't have to worry about it and can also remove all manual wake lock handling around playback.\n\nSimplified setting for CC button logic – Changing TrackSelectionParameters to say \"turn subtitles on/off\" was surprisingly hard to get right, so we added a simple boolean selectTextByDefault option for this use case.\n\n\nSimplify your media button preferences in MediaSession\nUntil now, defining your preferences for which buttons should show up in the media notification drawer on Android Auto or WearOS required defining custom commands and buttons, even if you simply wanted to trigger a standard player method.\nMedia3 1.9.0 has new functionality to make this a lot simpler – you can now define your media button preferences with a standard player command, requiring no custom command handling at all.\n\nsession.setMediaButtonPreferences(listOf(\n    CommandButton.Builder(CommandButton.ICON_FAST_FORWARD) // choose an icon\n      .setDisplayName(R.string.skip_forward)\n      .setPlayerCommand(Player.COMMAND_SEEK_FORWARD) // choose an action \n      .build()\n))\n\n\n\nMedia button preferences with fast forward button\n\nCompositionPlayer for real-time preview\nThe 1.9.0 release introduces CompositionPlayer under a new @ExperimentalApi annotation. The annotation indicates that it is available for experimentation, but is still under development. \n\nCompositionPlayer is a new component in the Media3 editing APIs designed for real-time preview of media edits. Built upon the familiar Media3 Player interface, CompositionPlayer allows users to see their changes in action before committing to the export process. It uses the same Composition object that you would pass to Transformer for exporting, streamlining the editing workflow by unifying the data model for preview and export.\nWe encourage you to start using CompositionPlayer and share your feedback, and keep an eye out for forthcoming posts and updates to the documentation for more details.\n\nInAppMuxer as a default muxer in Transformer\nTransformer now uses InAppMp4Muxer as the default muxer for writing media container files. Internally, InAppMp4Muxer depends on the Media3 Muxer module, providing consistent behaviour across all API versions. \n\nNote that while Transformer no longer uses the Android platform's MediaMuxer by default, you can still provide FrameworkMuxer.Factory via setMuxerFactory if your use case requires it.\n\n\nNew speed adjustment APIs\nThe 1.9.0 release simplifies speed adjustments APIs for media editing. We've introduced new methods directly on EditedMediaItem.Builder to control speed, making the API more intuitive. You can now change the speed of a clip by calling setSpeed(SpeedProvider provider) on the EditedMediaItem.Builder:\n\nval speedProvider = object : SpeedProvider {\n    override fun getSpeed(presentationTimeUs: Long): Float {\n        return speed\n    }\n\n    override fun getNextSpeedChangeTimeUs(timeUs: Long): Long {\n        return C.TIME_UNSET\n    }\n}\n\nEditedMediaItem speedEffectItem = EditedMediaItem.Builder(mediaItem)\n    .setSpeed(speedProvider)\n    .build()\n\n\nThis new approach replaces the previous method of using Effects#createExperimentalSpeedChangingEffects(), which we've deprecated and will remove in a future release.\n\nIntroducing track types for EditedMediaItemSequence\n\nIn the 1.9.0 release, EditedMediaItemSequence requires specifying desired output track types during sequence creation. This change ensures track handling is more explicit and robust across the entire Composition. \nThis is done via a new EditedMediaItemSequence.Builder constructor that accepts a set of track types (e.g., C.TRACK_TYPE_AUDIO, C.TRACK_TYPE_VIDEO). \nTo simplify creation, we've added new static convenience methods:\n\nEditedMediaItemSequence.withAudioFrom(List<EditedMediaItem>)\n\nEditedMediaItemSequence.withVideoFrom(List<EditedMediaItem>)\n\nEditedMediaItemSequence.withAudioAndVideoFrom(List<EditedMediaItem>)\n\nWe encourage you to migrate to the new constructor or the convenience methods for clearer and more reliable sequence definitions.\nExample of creating a video-only sequence:\n\nEditedMediaItemSequence videoOnlySequence =\n    EditedMediaItemSequence.Builder(setOf(C.TRACK_TYPE_VIDEO))\n        .addItem(editedMediaItem)\n        .build()\n\n\n---\nPlease get in touch via the Media3 issue Tracker if you run into any bugs, or if you have questions or feature requests. We look forward to hearing from you!"
  },
  {
    "source": "Android Developers Blog",
    "title": "Goodbye Mobile Only, Hello Adaptive: Three essential updates from 2025 for building adaptive apps",
    "link": "http://android-developers.googleblog.com/2025/12/goodbye-mobile-only-hello-adaptive.html",
    "pubDate": "2025-12-19T17:00:00.000Z",
    "contentSnippet": "Posted by Fahd Imtiaz – Product Manager, Android Developer\n\n\n\n\n\n\n\n\n\nGoodbye Mobile Only, Hello Adaptive: Three essential updates from 2025 for building adaptive apps\n\n\nIn 2025 the Android ecosystem has grown far beyond the phone. Today, developers have the opportunity to reach over 500 million active devices, including foldables, tablets, XR, Chromebooks, and compatible cars.\n\n\n\nThese aren't just additional screens; they represent a higher-value audience. We’ve seen that users who own both a phone and a tablet spend 9x more on apps and in-app purchases than those with just a phone. For foldable users, that average spend jumps to roughly 14x more*.\n\n\nThis engagement signals a necessary shift in development: goodbye mobile apps, hello adaptive apps.\n\n\n\n\n\n\n\nTo help you build for that future, we spent this year releasing tools that make adaptive the default way to build. Here are three key updates from 2025 designed to help you build these experiences.\n\n\nStandardizing adaptive behavior with Android 16\n\n\nTo support this shift, Android 16 introduced significant changes to how apps can restrict orientation and resizability. On displays of at least 600dp, manifest and runtime restrictions are ignored, meaning apps can no longer lock themselves to a specific orientation or size. Instead, they fill the entire display window, ensuring your UI scales seamlessly across portrait and landscape modes. \n\n\nBecause this means your app context will change more frequently, it’s important to verify that you are preserving UI state during configuration changes. While Android 16 offers a temporary opt-out to help you manage this transition, Android 17 (SDK37) will make this behavior mandatory. To ensure your app behaves as expected under these new conditions, use the resizable emulator in Android Studio to test your adaptive layouts today. \n\nSupporting screens beyond the tablet with Jetpack WindowManager 1.5.0\n\nAs devices evolve, our existing definitions of \"large\" need to evolve with them. In October, we released Jetpack WindowManager 1.5.0 to better support the growing number of very large screens and desktop environments.\n\n\nOn these surfaces, the standard \"Expanded\" layout, which usually fits two panes comfortably, often isn't enough. On a 27-inch monitor, two panes can look stretched and sparse, leaving valuable screen real estate unused. To solve this, WindowManager 1.5.0 introduced two new width window size classes: Large (1200dp to 1600dp) and Extra-large (1600dp+).\n\n\n\n\n\n\n\nThese new breakpoints signal when to switch to high-density interfaces. Instead of stretching a typical list-detail view, you can take advantage of the width to show three or even four panes simultaneously.  Imagine an email client that comfortably displays your folders, the inbox list, the open message, and a calendar sidebar, all in a single view. Support for these window size classes was added to Compose Material 3 adaptive in the 1.2 release. \n\n\nRethinking user journeys with Jetpack Navigation 3\n\n\nBuilding a UI that morphs from a single phone screen to a multi-pane tablet layout used to require complex state management.  This often meant forcing a navigation graph designed for single destinations to handle simultaneous views. First announced at I/O 2025, Jetpack Navigation 3 is now stable, introducing a new approach to handling user journeys in adaptive apps.\n\n\nBuilt for Compose, Nav3 moves away from the monolithic graph structure. Instead, it provides decoupled building blocks that give you full control over your back stack and state. This solves the single source of truth challenge common in split-pane layouts. Because Nav3 uses the Scenes API, you can display multiple panes simultaneously without managing conflicting back stacks, simplifying the transition between compact and expanded views.\n\n\nA foundation for an adaptive future\n\n\n\nThis year delivered the tools you need, from optimizing for expansive  layouts to the granular controls of WindowManager and Navigation 3. And, Android 16 began the shift toward truly flexible UI, with updates coming next year to deliver excellent adaptive experiences across all form factors. To learn more about adaptive development principles and get started, head over to d.android.com/adaptive-apps. \n\n\nThe tools are ready, and the users are waiting. We can’t wait to see what you build!\n\n\n*Source: internal Google data"
  },
  {
    "source": "Android Developers Blog",
    "title": "Bringing Androidify to Wear OS with Watch Face Push",
    "link": "http://android-developers.googleblog.com/2025/12/bringing-androidify-to-wear-os-with.html",
    "pubDate": "2025-12-18T17:00:00.000Z",
    "contentSnippet": "Posted by Garan Jenkin - Developer Relations Engineer\n\n\n\n\n\n\n\n\n\n\n\n\nA few months ago we relaunched Androidify as an app for generating personalized Android bots. Androidify transforms your selfie photo into a playful Android bot using Gemini and Imagen.\n\nHowever, given that Android spans multiple form factors, including our most recent addition, XR, we thought, how could we bring the fun of Androidify to Wear OS?\n\nAn Androidify watch face\n\nAs Androidify bots are highly-personalized, the natural place to showcase them is the watch face. Not only is it the most frequently visible surface but also the most personal surface, allowing you to represent who you are.\n\n\nPersonalized Androidify watch face, generated from selfie image\n\n\nAndroidify now has the ability to generate a watch face dynamically within the phone app and then send it to your watch, where it will automatically be set as your watch face. All of this happens within seconds!\nHigh-level design\nEnd-to-end flow for watch face creation and installation\nIn order to achieve the end-to-end experience, a number of technologies need to be combined together, as shown in this high-level design diagram.\nFirst of all, the user’s avatar is combined with a pre-existing Watch Face Format template, which is then packaged into an APK. This is validated - for reasons which will be explained! - and sent to the watch.\n\nOn being received by the watch, the new Watch Face Push API - part of Wear OS 6- is used to install and activate the watch face.\n\nLet’s explore the details:\nCreating the watch face templates\nThe watch face is created from a template, itself designed in Watch Face Designer. This is our new Figma plugin that allows you to create Watch Face Format watch faces directly within Figma.\n\n\nAn Androidify watch face template in Watch Face Designer\n\n\nThe plugin allows the watch face to be exported in a range of different ways, including as Watch Face Format (WFF) resources. These can then be easily incorporated as assets within the Androidify app, for dynamically building the finalized watch face.\nPackaging and validation\nOnce the template and avatar have been combined, the Portable Asset Compiler Kit (Pack) is used to assemble an APK.\nIn Androidify, Pack is used as a native library on the phone. For more details on how Androidify interfaces with the Pack library, see the GitHub repository.\nAs a final step before transmission, the APK is checked by the Watch Face Push validator.\nThis validator checks that the APK is suitable for installation. This includes checking the contents of the APK to ensure it is a valid watch face, as well as some performance checks. If it is valid, then the validator produces a token.\nThis token is required by the watch for installation.\nSending the watch face\nThe Androidify app on Wear OS uses WearableListenerService to listen for events on the Wearable Data Layer.\nThe phone app transfers the watch face by using a combination of MessageClient to set up the process, then ChannelClient to stream the APK.\nInstalling the watch face on the watch\nOnce the watch face is received on the Wear OS device, the Androidify app uses the new Watch Face Push API to install the watch face:\n\nval wfpManager = \n    WatchFacePushManagerFactory.createWatchFacePushManager(context)\nval response = wfpManager.listWatchFaces()\n\n\ntry {\n    if (response.remainingSlotCount > 0) {\n        wfpManager.addWatchFace(apkFd, token)\n    } else {\n        val slotId = response.installedWatchFaceDetails.first().slotId\n        wfpManager.updateWatchFace(slotId, apkFd, token)\n    }\n} catch (a: WatchFacePushManager.AddWatchFaceException) {\n    return WatchFaceInstallError.WATCH_FACE_INSTALL_ERROR\n} catch (u: WatchFacePushManager.UpdateWatchFaceException) {\n    return WatchFaceInstallError.WATCH_FACE_INSTALL_ERROR\n}\n\nAndroidify uses either the addWatchFace or updateWatchFace method, depending on the scenario: Watch Face Push defines a concept of “slots” - how many watch faces a given app can have installed at any time. For Wear OS 6, this value is in fact 1.\nAndroidify’s approach is to install the watch face if there is a free slot, and if not, any existing watch face is swapped out for the new one.\nSetting the active watch face\nInstalling the watch face programmatically is a great step, but Androidify seeks to ensure the watch face is also the active watch face. \nWatch Face Push introduces a new runtime permission which must be granted in order for apps to be able to achieve this:\ncom.google.wear.permission.SET_PUSHED_WATCH_FACE_AS_ACTIVE\n\nOnce this permission has been acquired, the wfpManager.setWatchFaceAsActive() method can be called, to set an installed watch face to being the active watch face.\nHowever, there are a number of considerations that Androidify has to navigate:\n\nsetWatchFaceAsActive can only be used once.\n\nSET_PUSHED_WATCH_FACE_AS_ACTIVE cannot be re-requested after being denied by the user.\n\nAndroidify might already be in control of the active watch face.\n\n\nFor more details see how Androidify implements the set active logic.\nGet started with Watch Face Push for Wear OS\nWatch Face Push is a versatile API, equally suited to enhancing Androidify as it is to building fully-featured watch face marketplaces.\n\nPerhaps you have an existing phone app and are looking for opportunities to further engage and delight your users?\nOr perhaps you’re an existing watch face developer looking to create your own community and gallery through releasing a marketplace app?\nTake a look at these resources:\n\nWatch Face Push\n\nWatch Face Format - Note also the upcoming policy changes relating to watch face publishing.\n\nWatch Face Designer\n\nAndroidify GitHub repository\n\nAndroidify Play Store listing\n\n\nAnd also check out the accompanying video for a greater-depth look at how we brought Androidify to Wear OS!\n\n\nWe’re looking forward to what you’ll create with Watch Face Push!"
  },
  {
    "source": "Android Developers Blog",
    "title": "Brighten Your Real-Time Camera Feeds with Low Light Boost",
    "link": "http://android-developers.googleblog.com/2025/12/brighten-your-real-time-camera-feeds.html",
    "pubDate": "2025-12-17T17:00:00.000Z",
    "contentSnippet": "Posted by Donovan McMurray, Developer Relations Engineer\n\n\n\n\nWe recently shared how Instagram enabled users to take stunning low light photos using Night Mode. That feature is perfect for still images, where there’s time to combine multiple exposures to create a high-quality static shot. But what about the moments that happen between the photos? Users need to interact with the camera more than just the moment the shutter button is pressed. They also use the preview to compose their scene or scan QR codes.\n\nToday, we’re diving into Low Light Boost (LLB), a powerful feature designed to brighten real-time camera streams. Unlike Night Mode, which requires a hold-still capture duration, Low Light Boost works instantaneously on your live preview and video recordings. LLB automatically adjusts how much brightening is needed based on available light, so it’s optimized for every environment.\n\nWith a recent update, LLB allows Instagram users to line up the perfect shot, and then their existing Night Mode implementation results in the same high quality low-light photos their users have been enjoying for over a year.\nWhy Real-time Brightness Matters\nWhile Night Mode aims to improve final image quality, Low Light Boost is intended for usability and interactivity in dark environments. Another important factor to consider is that – even though they work together very well – you can use LLB and Night Mode independently, and you’ll see with some of these use cases, LLB has value on its own when Night Mode photos aren’t needed. Here is how LLB improves the user experience:\n\nBetter Framing & Capture: In dimly lit scenes, a standard camera preview can be pitch black. LLB brightens the viewfinder, allowing users to actually see what they are framing before they hit the shutter button. For this experience, you can use Night Mode for the best quality low-light photo result, or you can let LLB give the user a “what you see is what you get” photo result.\n\nReliable Scanning: QR codes are ubiquitous, but scanning them in a dark restaurant or parking garage is often frustrating. With a significantly brighter camera feed, scanning algorithms can reliably detect and decode QR codes even in very dim environments.\n\nEnhanced Interactions: For apps involving live video interactions (like AI assistants or video calls) LLB increases the amount of perceivable information, ensuring the computer vision models have enough data to work with\n\nThe Difference in Instagram\n\n\n\n\n\n\nThe engineering team behind the Android Instagram app is always hard at work to provide a state-of-the-art camera experience for their users. You can see in the above example just what a difference LLB makes on a Pixel 10 Pro. \n\n\n\n\n\nIt’s easy to imagine the difference this makes in the user experience. If users aren’t able to see what they’re capturing, then there’s a higher chance they’ll abandon the capture. \n\n\n\n\n\nChoosing Your Implementation\nThere are two ways to implement Low Light Boost to provide the best experience across the widest range of devices:\n\nLow Light Boost AE Mode: This is a hardware-layer auto-exposure mode. It offers the highest quality and performance because it fine-tunes the Image Signal Processor (ISP) pipeline directly. Always check for this first.\n\nGoogle Low Light Boost: If the device doesn't support the AE mode, you can fall back to this software-based solution provided by Google Play services. It applies post-processing to the camera stream to brighten it. As an all-software solution, it is available on more devices, so this implementation helps you reach more devices with LLB.\n\nLow Light Boost AE Mode (Hardware)\nMechanism:\nThis mode is supported on devices running Android 15 and newer and requires the OEM to have implemented the support in HAL (currently available on Pixel 10 devices). It integrates directly with the camera's Image Signal Processor (ISP). If you set CaptureRequest.CONTROL_AE_MODE to CameraMetadata.CONTROL_AE_MODE_ON_LOW_LIGHT_BOOST_BRIGHTNESS_PRIORITY, the camera system takes control.\nBehavior:\nThe HAL/ISP analyzes the scene and adjusts sensor and processing parameters, often including increasing exposure time, to brighten the image. This can yield frames with a significantly improved signal-to-noise ratio (SNR) because the extended exposure time, rather than an increase in digital sensor gain (ISO), allows the sensor to capture more light information.\nAdvantage:\nPotentially better image quality and power efficiency as it leverages dedicated hardware pathways.\nTrade off:\nMay result in a lower frame rate in very dark conditions as the sensor needs more time to capture light. The frame rate can drop to as low as 10 FPS in very low light conditions.\nGoogle Low Light Boost (Software via Google Play Services)\nMechanism:\nThis solution, distributed as an optional module via Google Play services, applies post-processing to the camera stream. It uses a sophisticated realtime image enhancement technology called HDRNet.\nGoogle HDRNet:\nThis deep learning model analyzes the image at a lower resolution to predict a compact set of parameters (a bilateral grid). This grid then guides the efficient, spatially-varying enhancement of the full-resolution image on the GPU. The model is trained to brighten and improve image quality in low-light conditions, with a focus on face visibility.\nProcess Orchestration:\nThe HDRNet model and its accompanying logic are orchestrated by the Low Light Boost processor. This includes:\n\nScene Analysis:\nA custom calculator that estimates the true scene brightness using camera metadata (sensor sensitivity, exposure time, etc.) and image content. This analysis determines the boost level.\n\nHDRNet Processing:\nApplies the HDRNet model to brighten the frame. The model used is tuned for low light scenes and optimized for realtime performance.\n\nBlending:\nThe original and HDRNet processed frames are blended. The amount of blending applied is dynamically controlled by the scene brightness calculator, ensuring a smooth transition between boosted and unboosted states.\n\n\nAdvantage:\nWorks on a broader range of devices (currently supports Samsung S22 Ultra, S23 Ultra, S24 Ultra, S25 Ultra, and Pixel 6 through Pixel 9) without requiring specific HAL support. Maintains the camera's frame rate as it's a post-processing effect.\nTrade-off:\nAs a post-processing method, the quality is limited by the information present in the frames delivered by the sensor. It cannot recover details lost due to extreme darkness at the sensor level.\nBy offering both hardware and software pathways, Low Light Boost provides a scalable solution to enhance low-light camera performance across the Android ecosystem. Developers should prioritize the AE mode where available and use the Google Low Light Boost as a robust fallback.\nImplementing Low Light Boost in Your App\nNow let’s look at how to implement both LLB offerings. You can implement the following whether you use CameraX or Camera2 in your app. For the best results, we recommend implementing both Step 1 and Step 2.\nStep 1: Low Light Boost AE Mode\nAvailable on select devices running Android 15 and higher, LLB AE Mode functions as a specific Auto-Exposure (AE) mode.\n1. Check for Availability\nFirst, check if the camera device supports LLB AE Mode.\nval cameraInfo = cameraProvider.getCameraInfo(cameraSelector)\nval isLlbSupported = cameraInfo.isLowLightBoostSupported\n\n2. Enable the Mode\nIf supported, you can enable LLB AE Mode using CameraX’s CameraControl object.\n// After setting up your camera, use the CameraInfo object to enable LLB AE Mode.\ncamera = cameraProvider.bindToLifecycle(...)\n\nif (isLlbSupported) {\n  try {\n    // The .await() extension suspends the coroutine until the\n    // ListenableFuture completes. If the operation fails, it throws\n    // an exception which we catch below.\n    camera?.cameraControl.enableLowLightBoostAsync(true).await()\n  } catch (e: IllegalStateException) {\n    Log.e(TAG, \"Failed to enable low light boost: not available on this device or with the current camera configuration\", e)\n  } catch (e: CameraControl.OperationCanceledException) {\n    Log.e(TAG, \"Failed to enable low light boost: camera is closed or value has changed\", e)\n  }\n}\n3. Monitor the State\nJust because you requested the mode doesn't mean it's currently \"boosting.\" The system only activates the boost when the scene is actually dark. You can set up an Observer to update your UI (like showing a moon icon) or convert to a Flow using the extension function asFlow().\nif (isLlbSupported) {\n  camera?.cameraInfo.lowLightBoostState.asFlow().collectLatest { state ->\n    // Update UI accordingly\n    updateMoonIcon(state == LowLightBoostState.ACTIVE)\n  }\n}\n\nYou can read the full guide on Low Light Boost AE Mode here.\nStep 2: Google Low Light Boost\nFor devices that don't support the hardware AE mode, Google Low Light Boost acts as a powerful fallback. It uses a LowLightBoostSession to intercept and brighten the stream.\n1. Add Dependencies\nThis feature is delivered via Google Play services.\nimplementation(\"com.google.android.gms:play-services-camera-low-light-boost:16.0.1-beta06\")\n// Add coroutines-play-services to simplify Task APIs\nimplementation(\"org.jetbrains.kotlinx:kotlinx-coroutines-play-services:1.10.2\")\n2. Initialize the Client\nBefore starting your camera, use the LowLightBoostClient to ensure the module is installed and the device is supported.\nval llbClient = LowLightBoost.getClient(context)\n\n// Check support and install if necessary\nval isSupported = llbClient.isCameraSupported(cameraId).await()\nval isInstalled = llbClient.isModuleInstalled().await()\n\nif (isSupported && !isInstalled) {\n    // Trigger installation\n    llbClient.installModule(installCallback).await()\n}\n3. Create a LLB Session\nGoogle LLB processes each frame, so you must give your display Surface to the LowLightBoostSession, and it gives you back a Surface that has the brightening applied. For Camera2 apps, you can add the resulting Surface with CaptureRequest.Builder.addTarget(). For CameraX, this processing pipeline aligns best with the CameraEffect class, where you can apply the effect with a SurfaceProcessor and provide it back to your Preview with a SurfaceProvider, as seen in this code.\n// With a SurfaceOutput from SurfaceProcessor.onSurfaceOutput() and a\n// SurfaceRequest from Preview.SurfaceProvider.onSurfaceRequested(),\n// create a LLB Session.\nsuspend fun createLlbSession(surfaceRequest: SurfaceRequest, outputSurfaceForLlb: Surface) {\n  // 1. Create the LLB Session configuration\n  val options = LowLightBoostOptions(\n    outputSurfaceForLlb,\n    cameraId,\n    surfaceRequest.resolution.width,\n    surfaceRequest.resolution.height,\n    true // Start enabled\n  )\n\n  // 2. Create the session.\n  val llbSession = llbClient.createSession(options, callback).await()\n\n  // 3. Get the surface to use.\n  val llbInputSurface = llbSession.getCameraSurface()\n\n  // 4. Provide the surface to the CameraX Preview UseCase.\n  surfaceRequest.provideSurface(llbInputSurface, executor, resultListener)\n\n  // 5. Set the scene detector callback to monitor how much boost is being applied.\n  val onSceneBrightnessChanged = object : SceneDetectorCallback {\n    override fun onSceneBrightnessChanged(\n      session: LowLightBoostSession,\n      boostStrength: Float\n    ) {\n      // Monitor the boostStrength from 0 (no boosting) to 1 (maximum boosting)\n    }\n  }\n  llbSession.setSceneDetectorCallback(onSceneBrightnessChanged, null)\n}\n4. Pass in the Metadata\nFor the algorithm to work, it needs to analyze the camera's auto-exposure state. You must pass capture results back to the LLB session. In CameraX, this can be done by extending your Preview.Builder with Camera2Interop.Extender.setSessionCaptureCallback().\nCamera2Interop.Extender(previewBuilder).setSessionCaptureCallback(\n  object : CameraCaptureSession.CaptureCallback() {\n    override fun onCaptureCompleted(\n      session: CameraCaptureSession,\n      request: CaptureRequest,\n      result: TotalCaptureResult\n    ) {\n      super.onCaptureCompleted(session, request, result)\n      llbSession?.processCaptureResult(result)\n    }\n  }\n)\n\nDetailed implementation steps for the client and session can be found in the Google Low Light Boost guide.\nNext Steps\nBy implementing these two options, you ensure that your users can see clearly, scan reliably, and interact effectively, regardless of the lighting conditions.\nTo see these features in action within a complete, production-ready codebase, check out the Jetpack Camera App on GitHub. It implements both LLB AE Mode and Google LLB, giving you a reference for your own integration."
  },
  {
    "source": "Android Developers Blog",
    "title": "Build smarter apps with Gemini 3 Flash",
    "link": "http://android-developers.googleblog.com/2025/12/build-smarter-apps-with-gemini-3-flash.html",
    "pubDate": "2025-12-17T16:13:00.000Z",
    "contentSnippet": "Posted by Thomas Ezan, Senior Developer Relations Engineer\n\n\n\n\n\n\nToday, we're expanding the Gemini 3 model family with the release of Gemini 3 Flash, frontier intelligence built for speed at a fraction of the cost. You can start building with it immediately, as we’re officially launching Gemini 3 Flash on Firebase AI Logic. Available globally, you can securely access the Gemini 3 Flash preview model directly from your app via the Gemini Developer API or the Vertex AI Gemini API using Firebase AI Logic client SDKs. Gemini 3 Flash’s strong performance in reasoning, tool use, and multimodal capabilities is ideal for developers looking to do more complex video analysis, data extraction and visual Q&A.\nGemini 3 optimized for low-latency\nGemini 3 is our most intelligent model family to date. With the launch of Gemini 3 Flash, we are making that intelligence more accessible for low-latency and cost-effective use cases. While Gemini 3 Pro is designed for complex reasoning, Gemini 3 Flash is engineered to be significantly faster and more cost-effective for your production apps.\nSeamless integration with Firebase AI Logic\nJust like the Pro model, Gemini 3 Flash is available in preview directly through the Firebase AI Logic SDK. This means you can integrate it into your Android app without needing to do any complex server side setup.\nHere is how to add it to your Kotlin code:\n\n\nval model = Firebase.ai(backend = GenerativeBackend.googleAI())\n    .generativeModel(\n        modelName = \"gemini-3-flash-preview\")\n\nScale with Confidence\nIn addition, Firebase enables you to keep your growth secure and manageable with:\nAI Monitoring\nThe Firebase AI monitoring dashboard gives you visibility into latency, success rates, and costs, allowing you to slice data by model name to see exactly how the model performs.\n\n\n\n\n\n\nServer Prompt Templates\nYou can use server prompt templates to store your prompt and schema securely on Firebase servers instead of hardcoding them in your app binary. This capability ensures your sensitive prompts remain secure, prevents unauthorized prompt extraction, and allows for faster iteration without requiring app updates.\n---\nmodel: 'gemini-3-flash-preview'\ninput:\n  schema:\n    topic:\n      type: 'string'\n      minLength: 2\n      maxLength: 40\n    length:\n      type: 'number'\n      minimum: 1\n      maximum: 200\n    language:\n      type: 'string'\n---\n\n{{role \"system\"}}\nYou're a storyteller that tells nice and joyful stories with happy endings.\n\n{{role \"user\"}}\nCreate a story about {{topic}} with the length of {{length}} words in the {{language}} language.\n\nPrompt template defined on the Firebase Console  \n\nval generativeModel = Firebase.ai.templateGenerativeModel()\nval response = generativeModel.generateContent(\"storyteller-v10\",\n    mapOf(\n        \"topic\" to topic,\n        \"length\" to length,\n        \"language\" to language\n    )\n)\n_output.value = response.text\nCode snippet to access to the prompt template\n\nGemini 3 Flash for AI development assistance in Android Studio\nGemini 3 Flash is also available for AI assistance in Android Studio. While Gemini 3 Pro Preview is our best model for coding and agentic experiences, Gemini 3 Flash is engineered for speed, and great for common development tasks and questions.\n  \nThe new model is rolling out to developers using Gemini in Android Studio at no-cost (default model) starting today. For higher usage rate limits and longer sessions with Agent Mode, you can use an AI Studio API key to leverage the full capabilities of either Gemini 3 Flash or Gemini 3 Pro. We’re also rolling out Gemini 3 model family access with higher usage rate limits to developers who have Gemini Code Assist Standard or Enterprise licenses. Your IT administrator will need to enable access to preview models through the Google Cloud console.\nGet Started Today\nYou can start experimenting with Gemini 3 Flash via Firebase AI Logic today. Learn more about it in the Android and Firebase documentation. Try out any of the new Gemini 3 models in Android Studio for development assistance, and let us know what you think! As always you can follow us across LinkedIn, Blog,  YouTube, and X."
  },
  {
    "source": "Android Developers Blog",
    "title": "Notes from Google Play: A look back at the tools that powered your growth in 2025",
    "link": "http://android-developers.googleblog.com/2025/12/notes-from-google-play-2025.html",
    "pubDate": "2025-12-15T17:00:00.001Z",
    "contentSnippet": "Posted by Sam Bright – VP & GM, Google Play + Developer Ecosystem\n\n\n\n\n\n\n\n\n\n\nHi everyone,\n\nThank you for making 2025 another amazing year for Google Play.\n\nTogether, we’ve built Play into something much more than a store—it’s a dynamic ecosystem powered by your creativity. This year, our focus was ensuring Play continues to be the best destination for people to discover incredible content and enjoy rewarding gaming experiences.\n\nWe're incredibly proud of the progress we've made alongside you, and we’re excited to celebrate those who pushed the boundaries of what’s possible—like the winners of our Best of 2025 awards. Watch our recap video to see how we’ve made Play even more rewarding for your business, or read on for a more in-depth look back on the year.\n\n\n\n\nEvolving Play to be more than a store\n\n\nThis year, we focused on evolving Play into a true destination for discovery where billions of people around the world can find and enjoy experiences that make life more productive and delightful.\n\nMaking Play the best destination for your games business\nJust a few months ago, we shared our vision for a more unified experience that brings more fun to gaming. Today, players often jump between different platforms to discover, play, and get rewarded. Our goal is to connect these journeys to create the best experience for players and, consequently, grow your business. Our first steps include these key updates:\n\nA new Gamer Profile that tracks cross-game stats, streaks, and achievements, customizable with a Gen AI Avatar.\n\nIntegrated Rewards across mobile and PC that give players access to VIP experiences like our Four Days of Fantastic Rewards at San Diego Comic-Con,  Diamond District experience on Roblox, and Play’s own treasure-hunt mini-game Diamond Valley alongside new Play Games Leagues where players can compete in their favorite games, climb leaderboards, and win Play Points rewards.\n\nThe new Play Games Sidekick, a helpful in-game overlay that curates and organizes relevant gaming info, and provides direct access to Gemini Live for real-time AI-powered guidance in the game. We recently rolled out the open beta to developers, and we encourage you to start testing the sidekick in your games and share your feedback.\n\n\nIntegrated gameplay across devices is now fully realized as Google Play Games on PC has graduated from beta to general availability, solidifying our commitment to cross-platform play and making our catalog of over 200,000 titles available across mobile and PC.\n\n\n\n\n\nPlay Games Sidekick is a new in-game overlay that gives players instant access to their rewards, offers, and achievements, driving higher engagement for your game.\n\nTo help you get the most out of this unified gaming experience, we introduced the Google Play Games Level Up program, a new way to unlock greater success for your business. For titles that meet core user experience guidelines, you can unlock a powerful suite of benefits including the ability to:\n\n\nRe-engage players on the new You tab, a new personalized destination on the Play Store that is designed to help you re-engage and retain players by showcasing content and rewards from recently played games in one dedicated space. You can utilize engagement tools in Play Console to feature your latest events, offers, and updates.\n\nMaximize your game’s reach with prominent boosts across the store, including featuring opportunities, Play Points boosters and quests, and enhanced visibility on editorial surfaces like the Games Home and Play Points Home.\n\n\n\nYou tab is a personalized destination designed to help you re-engage and \nretain players by showcasing your latest events, offers, and updates.\n\n\nUnlocking more discovery and engagement for your apps and its content\nLast year, we shared our vision for a content-rich Google Play that has already delivered strong results. Year-over-year, Apps Home has seen over an 18% increase in average monthly visitors with apps seeing a 9% growth in acquisitions and double-digit growth* in app spend for those monetizing on Google Play. We introduced even more updates to elevate discovery and engagement on and off the store.\n\n\nCurated spaces, launched last year, have been a success, fostering routine engagement by delivering daily, locally relevant content (such as football highlights in Brazil, cricket in India, and comics in Japan) directly to the Apps Home. Building on this, we expanded to new categories and locations, including a new entertainment-focused space in Korea. \n\n\n\n\nCurated spaces make it easier to find and engage with local interests.\n\n\n\nWe significantly increased timely, relevant content on Google Play through Spotlight and new topic browse pages. Spotlight, located at the top of Apps Home, offers seasonal content feeds—like Taylor Swift’s recent album launch or holiday movie guides—in a new, immersive way to connect users with current cultural moments. Concurrently, new topic browse pages were integrated across the store in the U.S., Japan, and South Korea, allowing content deep dives into over 100,000 shows and movies.\n\n\n\n\nSpotlight offers an immersive experience connecting users\nwith relevant apps during current cultural moments.\n\nLast year, we introduced Engage SDK to help you deliver personalized content to users across surfaces and seamlessly guide them into the relevant in-app experiences. Integrating it unlocks surfaces like Collections, our immersive full-screen experience bringing content directly to the user's home screen. This year, we rolled out updates to expand your content’s reach even further:\n\n\nEngage SDK content expanded to the Play Store this summer, enabling seamless re-engagement across Apps Home and the new You tab.\n\nRolled out to more markets, including Brazil, Germany, India, Japan, and Korea.  \n\n\nSupporting you throughout your app lifecycle\n\n\nIn addition to evolving the store, we’ve continued to build on our powerful toolset to support you at every stage, from testing and release to growth and monetization.\n\nHelping you deliver high-quality, trusted user experiences\nWe launched key updates in Android Studio and Play Console to help you build more stable and compliant apps.\n\nPolicy insights in Android Studio help you catch potential violations early by showing in-context guidance, such as policy summaries and best practices, whenever code related to a Google Play policy is detected. \n\nYou can now halt fully rolled-out releases to stop the distribution of problematic app versions through Play Console and the Publishing API.\n\nWe also added new Android vitals performance metrics, including low memory kill metrics which provides device-specific insights to resolve stability problems and excessive partial wake lock metrics to help you address battery drain.\n\n\n\n\n\nNew Android vitals metrics help you resolve stability problems and address battery drain. \n\n\nBoosting your productivity and workflow\nWe refined the Play Console experience to make managing your app and your marketing content more efficient.\n\nWe put your most essential insights front and center with a redesigned app dashboard and overview pages.\n\nTo repurpose creative content across Play Console more easily, we launched an asset library that lets you upload from Google Drive, organize with tags, and crop existing visuals.\n\nYou can now automatically translate app strings with Gemini at no cost. This feature eliminates manual translation work for new releases, making updates seamless. You remain in full control with the ability to preview translations using a built-in emulator, and can easily edit or disable the service.\n\n\n\n\n\nTranslate app strings automatically with Gemini, while maintaining full control for previewing and editing.\n\nMaximizing your revenue with secure, frictionless payments\nWe introduced new features focused on driving purchases and maximizing subscription revenue globally.\n\nWe’re improving purchase conversion globally with over 800 million users now ready to buy. We launched features that encourage users to set up payment methods early, provide AI-powered payment method recommendations, and expanded our payment library to support more local payment methods globally.\n\nTo maximize recurring revenue from over 400 million paid subscriptions, we introduced multi-product checkout, allowing you to sell base subscriptions and add-ons under a simple, single transaction. \n\nTo combat churn, we began showcasing subscription benefits in more places and provided you with more flexible options like extended grace periods and account holds for declined payments, which has proven effective in reducing involuntary churn by an average of 10%*.\n\n\n\nTo help reduce voluntary churn, we’re showcasing your subscriptions benefits across Play.\n\n\nInvesting in our app and game community with developer programs\n\n\nWe’re proud to invest in programs for app and game companies around the world to help you grow and succeed on Play. \n\n\nGoogle Play Apps Accelerator: We’ve opened submissions for our program that will help early-stage app companies scale their business. Selected companies from over 80 eligible countries will join a 12-week accelerator starting in March 2026, where they can learn more about creating high-quality apps, go-to-market strategies, user acquisition, and more.\n\n\n\nSubmissions are still open for our 12-week accelerator, which starts in March 2026.  \nApply by January 7, 2026 for consideration.\n\n\nIndie Games Fund (Latin America): Now in its fourth year, this fund provides support to 10 promising game studios in Latin America with funding and hands-on support from Google Play. In October, we announced the 2025 recipients.\n\n\nChangGoo Program (South Korea): Now in its seventh year, this program works with over 100 Korean mobile app and game startups to foster their growth and expansion in collaboration with the Ministry of SMEs and Startups and the Korean Institute of Startup and Entrepreneurship Development (KISED).\n\n\nGoogle Play x Unity Game Developer Training Program (Indonesia): The third edition launched in April, offering a 6-month online curriculum, meetups, and mentorship for Indonesian game developers in partnership with Indonesia’s Ministry of Creative Economy and the Indonesian Gaming Association.\n\nGoogle Play x Unity Game Developer Training Program (India): The first India cohort kicked off in November with 500 aspiring and professional game developers. The 6-month journey provides online curriculum and meetups in partnership with GDAI and the govt of Tamil Nadu and Maharashtra.\n\n\nGoogle for Startups Accelerator program (India): We provided Seed to Series-A AI-powered app startups in India with insights on the latest AI advancements, mentorship, and expert guidance.\n\n\n\nProtecting your business and our ecosystem\n\n\nAt the heart of all the progress we’ve made this year is a foundation of trust and security. We're always innovating to make Play safer for everyone—so users can trust every app they download and so you can keep building a thriving business.\n\n\n\nTo offer stronger protection for your business and users, we continued to enhance the Play Integrity API and our anti-fraud systems. On average, apps using Play Integrity features see 80% lower unauthorized usage, and our efforts have safeguarded top apps using Play Billing from $2.9 billion in fraud and abuse in the last year.\n\n\nAutomatically fix user issues: New Play in-app remediation prompts in Play Integrity API automatically guide users to fix common problems like network issues, outdated Google Play Services, or device integrity flags, reducing integration complexity and getting users back to a good state faster.\n\nCombat repeat bad actors: Device recall is a powerful new tool that lets you store and recall limited data associated with a device, even if the device is reset, helping protect your business model from repeat bad actors.\n\nStrengthen revenue protection: We've introduced stronger protections against abuse, including refining pricing arbitrage detection and enhancing protection against free trial and intro pricing abuse for subscriptions, helping your business models remain profitable.\n\n\n\n\n\nWith in-app remediation prompts, Play automatically handles \na wide range of issues to guide your users back to a good state.\n\n\nFor a full breakdown of new ways we’re keeping the ecosystem safe, check out our deep-dive blog post here.\n\nThank you for your partnership\n\n\nThis is an incredible time for Google Play. We’ve made huge strides together – your passion, creativity, and feedback throughout 2025 has made Play that much stronger. We’re grateful to work alongside the best developer community in the world, and we look forward to unlocking even greater success together in the new year.\n\nHappy holidays!\nSam Bright\nVP & GM, Google Play + Developer Ecosystem\n\n\n\n\n\n\n* Source: Internal Google data"
  },
  {
    "source": "Android Developers Blog",
    "title": "18% Faster Compiles, 0% Compromises",
    "link": "http://android-developers.googleblog.com/2025/12/18-faster-compiles-0-compromises.html",
    "pubDate": "2025-12-15T17:00:00.000Z",
    "contentSnippet": "Posted by Santiago Aboy Solanes - Software Engineer, Vladimír Marko - Software Engineer\n\n\n\nThe Android Runtime (ART) team has reduced compile time by 18% without compromising the compiled code or any peak memory regressions. This improvement was part of our 2025 initiative to improve compile time without sacrificing memory usage or the quality of the compiled code.\nOptimizing compile-time speed is crucial for ART. For example, when just-in-time (JIT) compiling it directly impacts the efficiency of applications and overall device performance. Faster compilations reduce the time before the optimizations kick in, leading to a smoother and more responsive user experience. Furthermore, for both JIT and ahead-of-time (AOT), improvements in compile-time speed translate to reduced resource consumption during the compilation process, benefiting battery life and device thermals, especially on lower-end devices.\nSome of these compile-time speed improvements launched in the June 2025 Android release, and the rest will be available in the end-of-year release of Android. Furthermore, all Android users on versions 12 and above are eligible to receive these improvements through mainline updates.\nOptimizing the optimizing compiler\nOptimizing a compiler is always a game of trade-offs. You can't just get speed for free; you have to give something up. We set a very clear and challenging goal for ourselves: make the compiler faster, but do it without introducing memory regressions and, crucially, without degrading the quality of the code it produces. If the compiler is faster but the apps run slower, we've failed.\nThe one resource we were willing to spend was our own development time to dig deep, investigate, and find clever solutions that met these strict criteria. Let’s take a closer look at how we work to find areas to improve, as well as finding the right solutions to the various problems.\n\nFinding worthwhile possible optimizations\nBefore you can begin to optimize a metric, you have to be able to measure it. Otherwise, you can’t ever be sure if you improved it or not. Luckily for us, compile time speed is fairly consistent as long as you take some precautions like using the same device you use for measuring before and after a change, and making sure you don’t thermal throttle your device. On top of that, we also have deterministic measurements like compiler statistics that help us understand what’s going on under the hood.\nSince the resource we were sacrificing for these improvements was our development time, we wanted to be able to iterate as fast as we could. This meant that we grabbed a handful of representative apps (a mix of first-party apps, third-party apps, and the Android operating system itself) to prototype solutions. Later, we verified that the final implementation was worth it with both manual and automated testing in a widespread manner.\nWith that set of hand-picked apks we would trigger a manual compile locally, get a profile of the compilation, and use pprof to visualize where we are spending our time.\n\nExample of a profile’s flame graph in pprof\nThe pprof tool is very powerful and allows us to slice, filter, and sort the data to see, for example, which compiler phases or methods are taking most of the time. We will not go into detail about pprof itself; just know that if the bar is bigger then it means it took more time of the compilation.\nOne of these views is the “bottom up” one where you can see which methods are taking most of the time. In the image below we can see a method called Kill, accounting for over a 1% of the compile time. Some of the other top methods will also be discussed later in the blog post.\n\n\n\nBottom up view of a profile\nIn our optimizing compiler, there’s a phase called Global Value Numbering (GVN). You don’t have to worry about what it does as a whole, but the relevant part is to know that it has a method called `Kill` that it will delete some nodes according to a filter. This is time consuming as it has to iterate through all the nodes and check one by one. We noticed that there are some cases in which we know in advance that the check will be false, no matter the nodes we have alive at that point. In these cases, we can skip iterating altogether, bringing it from 1.023% down to ~0.3% and improving GVN’s runtime by ~15%.\nImplementing worthwhile optimizations\nWe covered how to measure and how to detect where the time is being spent, but this is only the beginning. The next step is how to optimize the time being spent compiling.\nUsually, in a case like the `Kill` one above we would take a look at how we iterate through the nodes and do it faster by, for example, doing things in parallel or improving the algorithm itself. In fact, that’s what we tried at first and only when we couldn’t find anything to do we had a “Wait a minute…” moment and saw that the solution was to (in some cases) not iterate at all! When doing these kinds of optimizations, it is easy to miss the forest for the trees.\nIn other cases, we used a handful of different techniques including:\n\nusing heuristics to decide whether an optimization will fail to produce worthwhile results and therefore can be skipped\n\nusing extra data structures to cache computed data\n\nchanging the current data structures to get a speed boost\n\nlazily computing results to avoid cycles in some cases\n\nuse the right abstraction - unnecessary features can slow down the code\n\navoid chasing a frequently used pointer through many loads\n\nHow do we know if the optimizations are worth pursuing?\nThat’s the neat part, you don’t. After detecting that an area is consuming a lot of compile time and after devoting development time to try to improve it, sometimes you can’t just find a solution. Maybe there’s nothing to do, it will take too long to implement, it will regress another metric significantly, increase code base complexity, etc. For every successful optimization that you can see in this blog post, know that there are countless others that just didn’t come to fruition.\nIf you are in a similar situation, try to estimate how much you are going to improve the metric by doing as little work as you can. This means, in order:\n\nEstimating with a metrics you have already collected, or just a gut feeling\n\nEstimating with a quick and dirty prototype\n\nImplement a solution.\n\nDon’t forget to consider estimating the drawbacks of your solution. For example, if you are going to rely on extra data structures, how much memory are you willing to use?\nDiving deeper\nWithout further ado, let’s look at some of the changes we implemented.\nWe implemented a change to optimize a method called FindReferenceInfoOf. This method was doing a linear search of a vector to find an entry. We updated that data structure to be indexed by the instruction’s id so that FindReferenceInfoOf would be O(1) instead of O(n). Also, we pre-allocated the vector to avoid resizing. We slightly increased memory as we had to add an extra field which counted how many entries we inserted in the vector, but it was a small sacrifice to make as the peak memory didn’t increase. This sped up our LoadStoreAnalysis phase by 34-66% which in turns gives ~0.5-1.8% compile time improvement.\nWe have a custom implementation of HashSet that we use in several places. Creating this data structure was taking a considerable amount of time and we found out why. Many years ago, this data structure was used in only a few places that were using very big HashSets and it was tweaked to be optimized for that. However, nowadays it was used in the opposite direction with only a few entries and with a short lifespan. This meant that we were wasting cycles by creating this huge HashSet but we only used it for a few entries before discarding it. With this change, we improved ~1.3-2% of compile time. As an added bonus, memory usage decreased by ~0.5-1% since we weren’t using as big data structures as before.\nWe improved ~0.5-1% of compile time by passing data structures by reference to the lambda to avoid copying them around. This was something that was missed in the original review and sat in our codebase for years. It was thanks to taking a look at the profiles in pprof that we noticed that these methods were creating and destroying a lot of data structures, which led us to investigate and optimize them.\nWe sped up the phase that writes the compiled output by caching computed values, which translated to ~1.3-2.8% of total compile time improvement. Sadly, the extra bookkeeping was too much and our automated testing alerted us of the memory regression. Later, we took a second look at the same code and implemented a new version which not only took care of the memory regression but also improved the compile time a further ~0.5-1.8%! In this second change we had to refactor and reimagine how this phase should work, in order to get rid of one of the two data structures.\nWe have a phase in our optimizing compiler which inlines function calls in order to get better performance. To choose which methods to inline we use both heuristics before we do any computation, and final checks after doing work but right before we finalize the inlining. If any of those detect that the inlining is not worth it (for example, too many new instructions would be added), then we don’t inline the method call.\nWe moved two checks from the “final checks” category to the “heuristic” category to estimate whether an inlining will succeed or not before we do any time-expensive computation. Since this is an estimate it is not perfect, but we verified that our new heuristics cover 99.9% of what was inlined before without affecting performance. One of these new heuristics was about the needed DEX registers (~0.2-1.3% improvement), and the other one about number of instructions (~2% improvement).\nWe have a custom implementation of a BitVector that we use in several places. We replaced the resizable BitVector class with a simpler BitVectorView for certain fixed-size bit vectors. This eliminates some indirections and run-time range checks and speeds up the construction of the bit vector objects.\nFurthermore, the BitVectorView class was templatized on the underlying storage type (instead of always using uint32_t as the old BitVector). This allows some operations, for example Union(), to process twice as many bits together on 64-bit platforms. The samples of the affected functions were reduced by more than 1% in total when compiling the Android OS. This was done across several changes [1, 2, 3, 4, 5, 6]\nIf we talked in detail about all the optimizations we would be here all day! If you are interested in some more optimizations, take a look at some other changes we implemented:\n\nAdd bookkeeping to improve compilation times by ~0.6-1.6%.\n\nLazily compute data to avoid cycles, if possible.\n\nRefactor our code to skip precomputing work when it will not be used.\n\nAvoid some dependent load chains when the allocator can be readily obtained from other places.\n\nAnother case of adding a check to avoid unnecessary work.\n\nAvoid frequent branching on register type (core/FP) in register allocator.\n\nMake sure some arrays are initialized at compile time. Don’t rely on clang to do it.\n\nClean up some loops. Use range loops that clang can optimize better because it does not need to reload the internal pointers of the container due to loop side effects. Avoid calling the virtual function `HInstruction::GetInputRecords()` in the loop via the inlined `InputAt(.)` for each input.\n\nAvoid Accept() functions for the visitor pattern by exploiting a compiler optimization.\n\nConclusion\nOur dedication to improving ART’s compile-time speed has yielded significant improvements, making Android more fluid and efficient while also contributing to better battery life and device thermals. By diligently identifying and implementing optimizations, we've demonstrated that substantial compile-time gains are possible without compromising memory usage or code quality.\nOur journey involved profiling with tools like pprof, a willingness to iterate, and sometimes even abandon less fruitful avenues. The collective efforts of the ART team have not only reduced compile time by a noteworthy percentage, but have also laid the groundwork for future advancements.\nAll of these improvements are available in the 2025 end-of-year Android update, and for Android 12 and above through mainline updates. We hope this deep dive into our optimization process provides valuable insights into the complexities and rewards of compiler engineering!"
  },
  {
    "source": "Android Developers Blog",
    "title": "Building a safer Android and Google Play, together",
    "link": "http://android-developers.googleblog.com/2025/12/building-safer-android-and-google-play.html",
    "pubDate": "2025-12-11T22:26:00.000Z",
    "contentSnippet": "Posted by Matthew Forsythe , Director, Product Management, App & Ecosystem Trust and Ron Aquino Sr. Director, Trust and Safety, Chrome, Android and Play\n\n\n\n\n\n\n\n\n\nEarlier this year, we reiterated our commitment to keeping Android and Google Play safe for everyone and maintaining a thriving environment where users can trust the apps they download and your business can flourish. We’ve heard your feedback clearly, from excited conversations at Play events around the world to the honest concerns on social media. You want simpler ways to make sure your apps are compliant and pass review, and need strong protections for your business so you can focus on growth and innovation. We are proud of the steps we’ve taken together this year, but know this is ongoing work in a  complex, ever-changing market. \n\n\nHere are key actions we’ve taken this year to simplify your development journey and strengthen protection.\n\nSimpler ways to build safer apps from the start\nThis year, we focused on making improvements to the app publishing experience by reducing friction points, from the moment you write code to submitting your app for review.\n\nPolicy guidance right where you code: We rolled out Play Policy Insights to all developers using Android Studio. This feature provides real-time, in-context guidance and policy warnings as you code, helping you proactively identify and resolve potential issues before you even submit your app for review.\n\nPre-review checks to help prevent app review surprises: Last year, we launched pre-review checks in Play Console so you can identify issues early, like incomplete policy declarations or crashes, and avoid rejections. This year, we expanded these checks for privacy policy links, login credential requirements, data deletion request links, inaccuracies in your Data safety form, and more.\n\n\nStronger protection for your business and users\nWe are committed to providing you with powerful ways to protect your apps and users from abuse. Beyond existing tools, programs, and the performance and security enhancement that comes with every Android release, we’ve also launched: \n\nAdvanced abuse and fraud protection: We made the Play Integrity API faster and more resilient, and introduced new features like Play remediation prompts and device recall in beta. Device recall is a powerful new tool that lets you store and recall limited data associated with a device, even if the device is reset, helping protect your business model from repeat bad actors.\n\nTools to keep kids safe: \n\n\nWe continued to invest in protecting children across Google products, including Google Play. New Play policy helps keep our youngest users safe globally by requiring apps with dating and gambling features to use Play Console tools to prevent minors from accessing them. Our enhanced Restrict Minor Access feature now blocks the users who we determine to be minors from searching for, downloading, or making purchases in apps that they shouldn’t have access to. \n\nWe’ve also been providing tools to developers to help meet significant new age verification regulatory requirements in applicable US states.\n\n\nMore ways to stop malware from snooping on your app: Android 16 provides a new, powerful defense in a single line of code: accessibilityDataSensitive. This flag lets you explicitly mark views in your app as containing sensitive data and block malicious apps from seeing or performing interactions on it. If you already use setFilterTouchesWhenObscured(true) to protect your app from tapjacking, your views are automatically treated as sensitive data for accessibility for an instant additional layer of defense with no extra work. \n\n\nSmoother policy compliance experience\nWe’re listening to your concerns and proactively working to make the experience of Play policy compliance and Android security requirements more transparent,  predictable, and accessible for all developers. You asked for clarity, fairness, and speed, and here is what we launched:\n\nMore support when you need it: Beyond the webinars and resources that we share, you told us you needed more direct policy help to understand requirements and get answers. Next week, we’ll add a direct way for you to reach our team about policy questions in your Play Console. You’ll be able to find this new, integrated support experience directly within your Play Console via the “Help” section. We also expanded the Google Play Developer Help Community to more languages, like Indonesian, Japanese, Korean, and Portuguese. \n\nClearer documentation: You asked for policy that’s easier to understand. To help you quickly grasp essential requirements, we've introduced a new Key Considerations section across several policies (like Permissions and Target API Level) and included concise \"Do's & Don'ts\" and easier-to-read summaries.\n\nMore transparent appeals process: We introduced a 180-day appeal window for account terminations. This allows us to prioritize and make decisions faster for developers who file appeals.\n\nAndroid developer verification design changes: To support a diverse range of users and developers, we’re taking action on your feedback. \n\n\nFirst, we’re creating a dedicated free account type to support students and hobbyists who want to build apps just for a small group, like family and friends. This means that you can share your creations to a limited number of devices without needing to go through the full developer verification process. \n\nWe’re also building a flow for experienced users to be able to install unverified apps. This is being carefully designed to balance providing choice with prioritizing security, including clear warnings so users fully understand the risks before choosing to bypass standard safety checks. \n\n\nThe improvements we made this year are only the beginning. Your feedback helps drive our roadmap, and it will continue to inform future refinements to our policies, tools, experiences, and ensuring Android and Google Play remain the safest and most trusted place for you to innovate and grow your business. \n\n\nThank you for being our partner in building the future of Android."
  },
  {
    "source": "Android Developers Blog",
    "title": "Enhancing Android security: Stop malware from snooping on your app data",
    "link": "http://android-developers.googleblog.com/2025/12/enhancing-android-security-stop-malware.html",
    "pubDate": "2025-12-11T17:00:00.001Z",
    "contentSnippet": "Posted by Bennet Manuel, Product Management, Android App Safety and Rob Clifford, Developer Relations\n\n\n\n\n\n\n\n\n\nSecurity is foundational to Android. We partner with you to keep the platform safe and protect user data by offering powerful security tools and features, like Credential Manager and FLAG_SECURE. Every Android release brings performance and security enhancements, and with Android 16, you can take simple, significant steps to strengthen your app’s defenses. Check out our video or continue reading to learn more about our enhanced protections for accessibility APIs.\n\n\n\nProtect your app from snooping with a single line of code\nWe’ve seen that bad actors sometimes try to exploit accessibility API features to read sensitive information, like passwords and financial details, directly from the screen and manipulate a user's device by injecting touches. To combat this, Android 16 provides a new, powerful defense in a single line of code: accessibilityDataSensitive.\n\n\n\nThe accessibilityDataSensitive flag allows you to explicitly mark a view or composable as containing sensitive data. When you set this flag to true on your app, you are essentially blocking potentially malicious apps from accessing your sensitive view data or performing interactions on it. Here is how it works: any app requesting accessibility permission that hasn't explicitly declared itself as a legitimate accessibility tool (isAccessibilityTool=true) is denied access to that view.\n\nThis simple but effective change helps to prevent malware from stealing information and performing unauthorized actions, all without impacting users’ experience of legitimate accessibility tools. Note: If an app is not an accessibility tool but requests accessibility permissions and sets isAccessibilityTool=true, Play will reject it and Google Play Protect will block it on user devices. \n\n\n\nAutomatic, enhanced security for setFilterTouchesWhenObscured protection\n\n\nWe’ve already integrated this new accessibilityDataSensitive security functionality with the existing setFilterTouchesWhenObscured method. \n\nIf you already use setFilterTouchesWhenObscured(true) to protect your app from tapjacking, your views are automatically treated as sensitive data for accessibility. By enhancing the setFilterTouchesWhenObscured method with accessibilityDataSensitive protections, we’re instantly giving everyone an additional layer of defense with no extra work.\n\n\nGetting started\n\nWe recommend that you use setFilterTouchesWhenObscured, or alternatively the accessibilityDataSensitive flag, on any screen that contains sensitive information, including login pages, payment flows, and any view displaying personal or financial data.\nFor Jetpack Compose\n\n\n\nsetFilterTouchesWhenObscured\n\naccessibilityDataSensitive\n\n\nval composeView = LocalView.current DisposableEffect(Unit) { composeView.filterTouchesWhenObscured = true onDispose { composeView.filterTouchesWhenObscured = false } }\n\nUse the semantics modifier to apply the sensitiveData property to a composable.\nBasicText { text = “Your password”,\n            modifier = Modifier.semantics {\n                sensitiveData = true }}\n\n\n\n\nFor View-based apps\nIn your XML layout, add the relevant attribute to the sensitive view.\n\n\n\nsetFilterTouchesWhenObscured\n\naccessibilityDataSensitive\n\n\n<TextView android:filterTouchesWhenObscured=\"true\" />\n\n<TextView android:accessibilityDataSensitive=\"true\" />\n\n\n\nAlternatively, you can set the property programmatically in Java or Kotlin:\n\n\n\nsetFilterTouchesWhenObscured\n\naccessibilityDataSensitive\n\n\nmyView.filterTouchesWhenObscured = true;\n\nmyView.isAccessibilityDataSensitive = true;\n\n\nmyView.setFilterTouchesWhenObscured(true)\n\nmyView.setAccessibilityDataSensitive(true);\n\n\n\nRead more about the accessibilityDataSensitive and setFilterTouchesWhenObscured flags in the Tapjacking guide.\n\n\nPartnering with developers to keep users safe\nWe worked with developers early to ensure this feature meets real-world needs and integrates smoothly into your workflow.\n\n\n\n\n\n\n \"We've always prioritized protecting our customers' sensitive financial data, which required us to build our own protection layer against accessibility-based malware. Revolut strongly supports the introduction of this new, official Android API, as it allows us to gradually move away from our custom code in favor of a robust, single-line platform defense.\"\n\n- Vladimir Kozhevnikov, Android Engineer at Revolut\n\n\nYou can play a crucial role in protecting your users from malicious accessibility-based attacks by adopting these features. We encourage all developers to integrate these features into their apps to help keep users safe.\nTogether, we can build a more secure and trustworthy experience for everyone."
  },
  {
    "source": "Android Developers Blog",
    "title": "#WeArePlay: How Matraquina helps non-verbal kids communicate",
    "link": "http://android-developers.googleblog.com/2025/12/weareplay-how-matraquina-helps-non.html",
    "pubDate": "2025-12-11T17:00:00.000Z",
    "contentSnippet": "Posted by Robbie McLachlan, Developer Marketing\n\n\n\n\n\n\n\n\n\nIn our latest #WeArePlay film, we meet Adriano, Wagner and Grazyelle. The trio are behind Matraquinha, an app helping thousands of non-verbal children in more than 80 countries communicate. Discover more about their inspiring story and the impact on their own son, Gabriel.\n\n\n\nWagner, you developed Matraquinha for a deeply personal reason: your son, Gabriel. Can you tell us what inspired you to create this app for him?\nMy wife and I adopted our son at 10 months. We later found out he couldn’t speak and received a diagnosis of Autism, so we started researching ways to communicate with him and vice versa. The idea started with drawings of objects and phrases on cards for him to point to things he wanted. We wanted to make this more digital and so, with my brother Adriano’s help, we developed the Matraquinha app. \nHow does the app work?\nWagner: The app has almost 250 drawings, like digital flashcards. The child points to a card and the app announces the name of the object, place or feeling. Parents then more clearly understand what their child needs. \nGrazyelle: As a mom, after Gabriel started using the app, he was able to communicate and that reduced his feeling of crisis a lot. Before, he would be frustrated. Now with the app, my son can tell me what he needs.\n\n\n\nMatraquinha started as a personal app for your family, but is now helping users in over 77 countries. How did you achieve this scale? \nAdriano: When my brother came to me with the idea, we thought it would be for our family and had no idea it would turn into a global resource for more families. In the first week, we had 1 download. By the next year, we had 100,000 downloads, all organic with no ads. It showed us how important the app was to help families communicate with their non-verbal children.\nAdriano: It’s truly incredible for us to be on Google Play because, even without being senior engineers, this tool gave us an opportunity—an entry point—to bring communication to other families. We use other tools like Firebase Analytics which lets us see which cards and categories people are using the most, this helps us when developing new versions.\n\n\n\n\nWhat is next for Matraquinha, and what features are you most excited about bringing to the community?\nWe are adding an extra 500 real images to the app, because kids are growing and no longer want drawings as they become teenagers. We’re also creating a board that has pronouns, nouns, and verbs. So say, a child wants to let the parents know they like to eat hamburgers, they can tap on the different words and create a sentence. This gives them even more independence. We are also exploring ways to use AI to make the app even more personal and pursuing the same goal: ensuring every child can be heard.\nDiscover other inspiring app and game founders featured in #WeArePlay."
  },
  {
    "source": "Kotlin Blog",
    "title": "How Mobile Development Teams Use Kotlin in 2025: Insights From a Certified Trainer",
    "link": "https://blog.jetbrains.com/kotlin/2025/12/how-mobile-development-teams-use-kotlin-in-2025/",
    "pubDate": "Fri, 19 Dec 2025 14:44:34 +0000",
    "contentSnippet": "This is the second guest post in a two-part series from José Luis González. José Luis has a PhD in software development and is a JetBrains-certified Kotlin Trainer, who works with developers and engineering teams to deepen their Kotlin skills and apply the language effectively in real projects. At Hyperskill, he runs Kotlin instructor-led training for […]"
  },
  {
    "source": "Kotlin Blog",
    "title": "The Ultimate Guide to Successfully Adopting Kotlin in a Java-Dominated Environment",
    "link": "https://blog.jetbrains.com/kotlin/2025/12/the-ultimate-guide-to-successfully-adopting-kotlin-in-a-java-dominated-environment/",
    "pubDate": "Thu, 18 Dec 2025 15:04:44 +0000",
    "contentSnippet": "Adopting Kotlin in a Java-centric company  is not about flipping a switch or rewriting everything “the right way”. It’s about people, timing, risk, and trust. Over the last four weeks, we’ve published a series of blog posts by Urs Peter, covering all of these aspects of migrating to Kotlin. In this post, we’ll tie the […]"
  },
  {
    "source": "Kotlin Blog",
    "title": "Industry Leaders on the KotlinConf’25 Stage: What Global Brands Built With Kotlin",
    "link": "https://blog.jetbrains.com/kotlin/2025/12/industry-leaders-on-the-kotlinconf25-stage/",
    "pubDate": "Wed, 17 Dec 2025 10:24:25 +0000",
    "contentSnippet": "The Kotlin ecosystem continues to grow among the world’s most recognized brands, including Meta, AWS, Duolingo, Uber, and others. At KotlinConf 2025, these companies took the stage to share practical, real-world engineering stories. KotlinConf is where developers can learn directly from the teams building products at global scale. Software engineers showcase how they are tackling […]"
  },
  {
    "source": "Kotlin Blog",
    "title": "Kotlin 2.3.0 Released",
    "link": "https://blog.jetbrains.com/kotlin/2025/12/kotlin-2-3-0-released/",
    "pubDate": "Tue, 16 Dec 2025 16:10:38 +0000",
    "contentSnippet": "The Kotlin 2.3.0 release is out! This version includes new language features, stable ones, and other features now enabled by default. This release also brings tooling updates, performance improvements for different platforms, and important fixes. Here are some additional highlights from this release: For the complete list of changes, refer to What’s new in Kotlin […]"
  },
  {
    "source": "Kotlin Blog",
    "title": "Building AI Agents in Kotlin – Part 3: Under Observation",
    "link": "https://blog.jetbrains.com/ai/2025/12/building-ai-agents-in-kotlin-part-3-under-observation/",
    "pubDate": "Fri, 12 Dec 2025 11:58:45 +0000",
    "contentSnippet": "Previously in this series: Two articles in, and our coding agent can already do quite a bit. It can explore projects, read and write code, execute shell commands, and run tests. Adding a definition of done (DoD) in our last article gave it the feedback loop it needed – the agent now iterates until all […]"
  },
  {
    "source": "Kotlin Blog",
    "title": "A Better Way to Explore kotlinx-benchmark Results with Kotlin Notebooks",
    "link": "https://blog.jetbrains.com/kotlin/2025/12/a-better-way-to-explore-kotlinx-benchmark-results-with-kotlin-notebooks/",
    "pubDate": "Thu, 11 Dec 2025 16:02:35 +0000",
    "contentSnippet": "Benchmarking is an important part of writing efficient Kotlin code. The kotlinx-benchmark library helps you measure and compare performance across different implementations or hardware configurations. However, raw text results only take you so far. Sometimes you need to visualize your data, not just read it. That’s where Kotlin notebooks come in. Kotlin notebooks combine the […]"
  },
  {
    "source": "Kotlin Blog",
    "title": "How Backend Development Teams Use Kotlin in 2025: Insights from a Certified Trainer",
    "link": "https://blog.jetbrains.com/kotlin/2025/12/how-backend-development-teams-use-kotlin-in-2025/",
    "pubDate": "Tue, 09 Dec 2025 23:52:47 +0000",
    "contentSnippet": "This is the first guest post in a two-part series from José Luis González. José Luis has a PhD in software development and is a JetBrains-certified Kotlin Trainer, who works with developers and engineering teams to deepen their Kotlin skills and apply the language effectively in real projects. At Hyperskill, he runs Kotlin instructor-led training […]"
  },
  {
    "source": "Kotlin Blog",
    "title": "Kubernetes Made Simple: A Guide for JVM Developers",
    "link": "https://blog.jetbrains.com/kotlin/2025/12/kubernetes-made-simple-a-guide-for-jvm-developers/",
    "pubDate": "Mon, 08 Dec 2025 13:28:13 +0000",
    "contentSnippet": "This article was written by an external contributor. Kubernetes is a container orchestration system for deploying, scaling, and managing containerized applications. If you build services on the Java virtual machine (JVM), you likely know that most microservices run on Kubernetes. Kubernetes has become the de facto standard for running containerized microservices at scale. However, Kubernetes […]"
  },
  {
    "source": "Kotlin Blog",
    "title": "Advent of Code 2025: A Kotlin Playground",
    "link": "https://blog.jetbrains.com/kotlin/2025/11/advent-of-code-in-kotlin-2025/",
    "pubDate": "Fri, 28 Nov 2025 15:18:12 +0000",
    "contentSnippet": "Join us live for five days of Advent of Code puzzles and five Kotlin livestreams on December 1–5. Solve the puzzles in Kotlin with help from Kotlin team experts and fun community guests, climb the leaderboard, learn new tricks, and have a chance to win some prizes along the way! Here’s what we have planned […]"
  },
  {
    "source": "Kotlin Blog",
    "title": "Kodee’s Kotlin Roundup: Too Much News to Keep Quiet About",
    "link": "https://blog.jetbrains.com/kotlin/2025/11/kodees-kotlin-roundup-november-edition/",
    "pubDate": "Wed, 26 Nov 2025 11:34:28 +0000",
    "contentSnippet": "I’ve gathered the latest Kotlin highlights for you – from the Kotlin Reddit AMA and documentation updates to learning programs and Google Summer of Code 2025 projects. Whether you’re here to stay up to date or just looking for something interesting to explore, there’s plenty to dive into. Where You Can Learn More YouTube Highlights"
  },
  {
    "source": "Hugging Face Blog",
    "title": "Small Yet Mighty: Improve Accuracy In Multimodal Search and Visual Document Retrieval with Llama Nemotron RAG Models ",
    "link": "https://huggingface.co/blog/nvidia/llama-nemotron-vl-1b",
    "pubDate": "Tue, 06 Jan 2026 21:16:17 GMT",
    "contentSnippet": ""
  },
  {
    "source": "Hugging Face Blog",
    "title": "Introducing Falcon-H1-Arabic: Pushing the Boundaries of Arabic Language AI with Hybrid Architecture",
    "link": "https://huggingface.co/blog/tiiuae/falcon-h1-arabic",
    "pubDate": "Mon, 05 Jan 2026 09:16:51 GMT",
    "contentSnippet": ""
  },
  {
    "source": "Hugging Face Blog",
    "title": "NVIDIA brings agents to life with DGX Spark and Reachy Mini",
    "link": "https://huggingface.co/blog/nvidia-reachy-mini",
    "pubDate": "Mon, 05 Jan 2026 00:00:00 GMT",
    "contentSnippet": ""
  },
  {
    "source": "Hugging Face Blog",
    "title": "AprielGuard: A Guardrail for Safety and Adversarial Robustness in Modern LLM Systems",
    "link": "https://huggingface.co/blog/ServiceNow-AI/aprielguard",
    "pubDate": "Tue, 23 Dec 2025 14:07:35 GMT",
    "contentSnippet": ""
  },
  {
    "source": "Hugging Face Blog",
    "title": "Tokenization in Transformers v5: Simpler, Clearer, and More Modular",
    "link": "https://huggingface.co/blog/tokenizers",
    "pubDate": "Thu, 18 Dec 2025 00:00:00 GMT",
    "contentSnippet": ""
  },
  {
    "source": "Hugging Face Blog",
    "title": "The Open Evaluation Standard: Benchmarking NVIDIA Nemotron 3 Nano with NeMo Evaluator",
    "link": "https://huggingface.co/blog/nvidia/nemotron-3-nano-evaluation-recipe",
    "pubDate": "Wed, 17 Dec 2025 13:22:18 GMT",
    "contentSnippet": ""
  },
  {
    "source": "Hugging Face Blog",
    "title": "CUGA on Hugging Face: Democratizing Configurable AI Agents",
    "link": "https://huggingface.co/blog/ibm-research/cuga-on-hugging-face",
    "pubDate": "Mon, 15 Dec 2025 16:01:04 GMT",
    "contentSnippet": ""
  },
  {
    "source": "Hugging Face Blog",
    "title": "New in llama.cpp: Model Management",
    "link": "https://huggingface.co/blog/ggml-org/model-management-in-llamacpp",
    "pubDate": "Thu, 11 Dec 2025 15:47:44 GMT",
    "contentSnippet": ""
  },
  {
    "source": "Hugging Face Blog",
    "title": "Codex is Open Sourcing AI models",
    "link": "https://huggingface.co/blog/hf-skills-training-codex",
    "pubDate": "Thu, 11 Dec 2025 00:00:00 GMT",
    "contentSnippet": ""
  },
  {
    "source": "Hugging Face Blog",
    "title": "Introducing swift-huggingface: The Complete Swift Client for Hugging Face",
    "link": "https://huggingface.co/blog/swift-huggingface",
    "pubDate": "Fri, 05 Dec 2025 00:00:00 GMT",
    "contentSnippet": ""
  },
  {
    "source": "Google AI Blog",
    "title": "Generative AI to quantify uncertainty in weather forecasting",
    "link": "http://blog.research.google/2024/03/generative-ai-to-quantify-uncertainty.html",
    "pubDate": "2024-03-29T18:03:00.000Z",
    "contentSnippet": "Posted by Lizao (Larry) Li, Software Engineer, and Rob Carver, Research Scientist, Google Research\n\n\n\n\nAccurate weather forecasts can have a direct impact on people’s lives, from helping make routine decisions, like what to pack for a day’s activities, to informing urgent actions, for example, protecting people in the face of hazardous weather conditions. The importance of accurate and timely weather forecasts will only increase as the climate changes. Recognizing this, we at Google have been investing in weather and climate research to help ensure that the forecasting technology of tomorrow can meet the demand for reliable weather information. Some of our recent innovations include MetNet-3, Google's high-resolution forecasts up to 24-hours into the future, and GraphCast, a weather model that can predict weather up to 10 days ahead.\n\n \n\n\nWeather is inherently stochastic. To quantify the uncertainty, traditional methods rely on physics-based simulation to generate an ensemble of forecasts. However, it is computationally costly to generate a large ensemble so that rare and extreme weather events can be discerned and characterized accurately.  \n\n\nWith that in mind, we are excited to announce our latest innovation designed to accelerate progress in weather forecasting, Scalable Ensemble Envelope Diffusion Sampler (SEEDS), recently published in Science Advances. SEEDS is a generative AI model that can efficiently generate ensembles of weather forecasts at scale at a small fraction of the cost of traditional physics-based forecasting models. This technology opens up novel opportunities for weather and climate science, and it represents one of the first applications to weather and climate forecasting of probabilistic diffusion models, a generative AI technology behind recent advances in media generation.\n\n \n\nThe need for probabilistic forecasts: the butterfly effect\nAmerican Association for the Advancement of Science meeting in Washington, D.C., MIT meteorology professor Ed Lorenz gave a talk entitled, “Does the Flap of a Butterfly's Wings in Brazil Set Off a Tornado in Texas?” which contributed to the term “butterfly effect”. He was building on his earlier, landmark 1963 paper where he examined the feasibility of “very-long-range weather prediction” and described how errors in initial conditions grow exponentially when integrated in time with numerical weather prediction models. This exponential error growth, known as chaos, results in a deterministic predictability limit that restricts the use of individual forecasts in decision making, because they do not quantify the inherent uncertainty of weather conditions. This is particularly problematic when forecasting extreme weather events, such as hurricanes, heatwaves, or floods.\n\n\nRecognizing the limitations of deterministic forecasts, weather agencies around the world issue probabilistic forecasts. Such forecasts are based on ensembles of deterministic forecasts, each of which is generated by including synthetic noise in the initial conditions and stochasticity in the physical processes. Leveraging the fast error growth rate in weather models, the forecasts in an ensemble are purposefully different: the initial uncertainties are tuned to generate runs that are as different as possible and the stochastic processes in the weather model introduce additional differences during the model run. The error growth is mitigated by averaging all the forecasts in the ensemble and the variability in the ensemble of forecasts quantifies the uncertainty of the weather conditions.\n\n\nWhile effective, generating these probabilistic forecasts is computationally costly. They require running highly complex numerical weather models on massive supercomputers multiple times. Consequently, many operational weather forecasts can only afford to generate ~10–50 ensemble members for each forecast cycle. This is a problem for users concerned with the likelihood of rare but high-impact weather events, which typically require much larger ensembles to assess beyond a few days. For instance, one would need a 10,000-member ensemble to forecast the likelihood of events with 1% probability of occurrence with a relative error less than 10%. Quantifying the probability of such extreme events could be useful, for example, for emergency management preparation or for energy traders.\n\n \n\nSEEDS: AI-enabled advances\npaper, we present the Scalable Ensemble Envelope Diffusion Sampler (SEEDS), a generative AI technology for weather forecast ensemble generation. SEEDS is based on denoising diffusion probabilistic models, a state-of-the-art generative AI method pioneered in part by Google Research.\n\n\nSEEDS can generate a large ensemble conditioned on as few as one or two forecasts from an operational numerical weather prediction system. The generated ensembles not only yield plausible real-weather–like forecasts but also match or exceed physics-based ensembles in skill metrics such as the rank histogram, the root-mean-squared error (RMSE), and the continuous ranked probability score (CRPS). In particular, the generated ensembles assign more accurate likelihoods to the tail of the forecast distribution, such as ±2σ and ±3σ weather events. Most importantly, the computational cost of the model is negligible when compared to the hours of computational time needed by supercomputers to make a forecast. It has a throughput of 256 ensemble members (at 2° resolution) per 3 minutes on Google Cloud TPUv3-32 instances and can easily scale to higher throughput by deploying more accelerators. \n\n\n\n\nSEEDS generates an order-of-magnitude more samples to in-fill distributions of weather patterns.\n\nGenerating plausible weather forecasts\nGlobal Ensemble Forecast System, GEFS) for a particular date during the 2022 European heat waves. We also compare the results to the forecasts from a Gaussian model that predicts the univariate mean and standard deviation of each atmospheric field at each location, a common and computationally efficient but less sophisticated data-driven approach. This Gaussian model is meant to characterize the output of pointwise post-processing, which ignores correlations and treats each grid point as an independent random variable. In contrast, a real weather map would have detailed correlational structures. \n\n\nBecause SEEDS directly models the joint distribution of the atmospheric state, it realistically captures both the spatial covariance and the correlation between mid-tropospheric geopotential and mean sea level pressure, both of which are closely related and are commonly used by weather forecasters for evaluation and verification of forecasts. Gradients in the mean sea level pressure are what drive winds at the surface, while gradients in mid-tropospheric geopotential create upper-level winds that move large-scale weather patterns. \n\n\nThe generated samples from SEEDS shown in the figure below (frames Ca–Ch) display a geopotential trough west of Portugal with spatial structure similar to that found in the operational U.S. forecasts or the reanalysis based on observations. Although the Gaussian model predicts the marginal univariate distributions adequately, it fails to capture cross-field or spatial correlations. This hinders the assessment of the effects that these anomalies may have on hot air intrusions from North Africa, which can exacerbate heat waves over Europe.\n\n\n\n\nStamp maps over Europe on 2022/07/14 at 0:00 UTC. The contours are for the mean sea level pressure (dashed lines mark isobars below 1010 hPa) while the heatmap depicts the geopotential height at the 500 hPa pressure level. (A) The ERA5 reanalysis, a proxy for real observations. (Ba-Bb) 2 members from the 7-day U.S. operational forecasts used as seeds to our model. (Ca-Ch) 8 samples drawn from SEEDS. (Da-Dh) 8 non-seeding members from the 7-day U.S. operational ensemble forecast. (Ea-Ed) 4 samples from a pointwise Gaussian model parameterized by the mean and variance of the entire U.S. operational ensemble.\n\nCovering extreme events more accurately  \n\n\nSEEDS provides better statistical coverage of the 2022/07/14 European extreme heat event, denoted by the brown star . Each plot shows the values of the total column-integrated water vapor (TCVW) vs. temperature over a grid point near Lisbon, Portugal from 16,384 samples generated by our models, shown as green dots, conditioned on 2 seeds (blue squares) taken from the 7-day U.S. operational ensemble forecasts (denoted by the sparser brown triangles). The valid forecast time is 1:00 local time. The solid contour levels correspond to iso-proportions of the kernel density of SEEDS, with the outermost one encircling 95% of the mass and 11.875% between each level.\n\n \n\nConclusion and future outlook\n \n\nAcknowledgements\nAll SEEDS authors, Lizao Li, Rob Carver, Ignacio Lopez-Gomez, Fei Sha and John Anderson, co-authored this blog post, with Carla Bromberg as Program Lead. We also thank Tom Small who designed the animation. Our colleagues at Google Research have provided invaluable advice to the SEEDS work. Among them, we thank Leonardo Zepeda-Núñez, Zhong Yi Wan, Stephan Rasp, Stephan Hoyer, and Tapio Schneider for their inputs and useful discussion. We thank Tyler Russell for additional technical program management, as well as Alex Merose for data coordination and support. We also thank Cenk Gazen, Shreya Agrawal, and Jason Hickey for discussions in the early stage of the SEEDS work."
  },
  {
    "source": "Google AI Blog",
    "title": "AutoBNN: Probabilistic time series forecasting with compositional bayesian neural networks",
    "link": "http://blog.research.google/2024/03/autobnn-probabilistic-time-series.html",
    "pubDate": "2024-03-28T20:53:00.000Z",
    "contentSnippet": "Posted by Urs Köster, Software Engineer, Google Research\n\n\n\n\nTime series problems are ubiquitous, from forecasting weather and traffic patterns to understanding economic trends. Bayesian approaches start with an assumption about the data's patterns (prior probability), collecting evidence (e.g., new time series data), and continuously updating that assumption to form a posterior probability distribution. Traditional Bayesian approaches like Gaussian processes (GPs) and Structural Time Series are extensively used for modeling time series data, e.g., the commonly used Mauna Loa CO2 dataset. However, they often rely on domain experts to painstakingly select appropriate model components and may be computationally expensive. Alternatives such as neural networks lack interpretability, making it difficult to understand how they generate forecasts, and don't produce reliable confidence intervals. \n\n\n\nTo that end, we introduce AutoBNN, a new open-source package written in JAX. AutoBNN automates the discovery of interpretable time series forecasting models, provides high-quality uncertainty estimates, and scales effectively for use on large datasets. We describe how AutoBNN combines the interpretability of traditional probabilistic approaches with the scalability and flexibility of neural networks.\n\n\n\n    \nAutoBNN\nline of research that over the past decade has yielded improved predictive accuracy by modeling time series using GPs with learned kernel structures. The kernel function of a GP encodes assumptions about the function being modeled, such as the presence of trends, periodicity or noise.  With learned GP kernels, the kernel function is defined compositionally: it is either a base kernel (such as Linear, Quadratic, Periodic, Matérn or ExponentiatedQuadratic) or a composite that combines two or more kernel functions using operators such as Addition, Multiplication, or ChangePoint. This compositional kernel structure serves two related purposes. First, it is simple enough that a user who is an expert about their data, but not necessarily about GPs, can construct a reasonable prior for their time series. Second, techniques like Sequential Monte Carlo can be used for discrete searches over small structures and can output interpretable results.\nBayesian neural networks (BNNs) while retaining the compositional kernel structure. A BNN is a neural network with a probability distribution over weights rather than a fixed set of weights. This induces a distribution over outputs, capturing uncertainty in the predictions. BNNs bring the following advantages over GPs: First, training large GPs is computationally expensive, and traditional training algorithms scale as the cube of the number of data points in the time series. In contrast, for a fixed width, training a BNN will often be approximately linear in the number of data points. Second, BNNs lend themselves better to GPU and TPU hardware acceleration than GP training operations. Third, compositional BNNs can be easily combined with traditional deep BNNs, which have the ability to do feature discovery. One could imagine \"hybrid\" architectures, in which users specify a top-level structure of Add(Linear, Periodic, Deep), and the deep BNN is left to learn the contributions from potentially high-dimensional covariate information.\n\n\n\nHow might one translate a GP with compositional kernels into a BNN then? A single layer neural network will typically converge to a GP as the number of neurons (or \"width\") goes to infinity. More recently, researchers have discovered a correspondence in the other direction — many popular GP kernels (such as Matern, ExponentiatedQuadratic, Polynomial or Periodic) can be obtained as infinite-width BNNs with appropriately chosen activation functions and weight distributions. Furthermore, these BNNs remain close to the corresponding GP even when the width is very much less than infinite. For example, the figures below show the difference in the covariance between pairs of observations, and regression results of the true GPs and their corresponding width-10 neural network versions.\n\n\n\n\nComparison of Gram matrices between true GP kernels (top row) and their width 10 neural network approximations (bottom row).\n\n\n\nComparison of regression results between true GP kernels (top row) and their width 10 neural network approximations (bottom row).\n\nBNN analogues of the Addition and Multiplication operators over GPs, and input warping to produce periodic kernels. BNN addition is straightforwardly given by adding the outputs of the component BNNs. BNN multiplication is achieved by multiplying the activations of the hidden layers of the BNNs and then applying a shared dense layer. We are therefore limited to only multiplying BNNs with the same hidden width.\n\n\n\n    \nUsing AutoBNN\npackage is available within Tensorflow Probability. It is implemented in JAX and uses the flax.linen neural network library. It implements all of the base kernels and operators discussed so far (Linear, Quadratic, Matern, ExponentiatedQuadratic, Periodic, Addition, Multiplication) plus one new kernel and three new operators:  \n\n\n\n\na OneLayer kernel, a single hidden layer ReLU BNN,\n\n\na ChangePoint operator that allows smoothly switching between two kernels,\n\n\na LearnableChangePoint operator which is the same as ChangePoint except position and slope are given prior distributions and can be learnt from the data, and\n\n\na WeightedSum operator.\n\n\n\n\n\nWeightedSum combines two or more BNNs with learnable mixing weights, where the learnable weights follow a Dirichlet prior. By default, a flat Dirichlet distribution with concentration 1.0 is used.\n\n\n\nWeightedSums allow a \"soft\" version of structure discovery, i.e., training a linear combination of many possible models at once. In contrast to structure discovery with discrete structures, such as in AutoGP, this allows us to use standard gradient methods to learn structures, rather than using expensive discrete optimization. Instead of evaluating potential combinatorial structures in series, WeightedSum allows us to evaluate them in parallel. \n\n\n\nTo easily enable exploration, AutoBNN defines a number of model structures that contain either top-level or internal WeightedSums. The names of these models can be used as the first parameter in any of the estimator constructors, and include things like sum_of_stumps (the WeightedSum over all the base kernels) and sum_of_shallow (which adds all possible combinations of base kernels with all operators).\n\n\nIllustration of the sum_of_stumps model. The bars in the top row show the amount by which each base kernel contributes, and the bottom row shows the function represented by the base kernel. The resulting weighted sum is shown on the right.\n\nM3 dataset. The six base structures were ExponentiatedQuadratic (which is the same as the Radial Basis Function kernel, or RBF for short), Matern, Linear, Quadratic, OneLayer and Periodic kernels. The figure shows the MAP estimates of their weights over an ensemble of 32 particles. All of the high likelihood particles gave a large weight to the Periodic component, low weights to Linear, Quadratic and OneLayer, and a large weight to either RBF or Matern.\n\n\n\n\n\n\nParallel coordinates plot of the MAP estimates of the base kernel weights over 32 particles. The sum_of_stumps model was trained on the N374 series from the M3 dataset (insert in blue). Darker lines correspond to particles with higher likelihoods.\n\nWeightedSums as the inputs to other operators, it is possible to express rich combinatorial structures, while keeping models compact and the number of learnable weights small. As an example, we include the sum_of_products model (illustrated in the figure below) which first creates a pairwise product of two WeightedSums, and then a sum of the two products. By setting some of the weights to zero, we can create many different discrete structures. The total number of possible structures in this model is 216, since there are 16 base kernels that can be turned on or off. All these structures are explored implicitly by training just this one model.\n\n\n\n\n\nIllustration of the \"sum_of_products\" model. Each of the four WeightedSums have the same structure as the \"sum_of_stumps\" model.\n\nPeriodic and either the Matern or ExponentiatedQuadratic) lead to overfitting on many datasets. To prevent this, we have defined model classes like sum_of_safe_shallow that exclude such products when performing structure discovery with WeightedSums.\n\n\n\nFor training, AutoBNN provides AutoBnnMapEstimator and AutoBnnMCMCEstimator to perform MAP and MCMC inference, respectively. Either estimator can be combined with any of the six likelihood functions, including four based on normal distributions with different noise characteristics for continuous data and two based on the negative binomial distribution for count data.  \n\n\n\n\n\n\nResult from running AutoBNN on the Mauna Loa CO2 dataset in our example colab. The model captures the trend and seasonal component in the data. Extrapolating into the future, the mean prediction slightly underestimates the actual trend, while the 95% confidence interval gradually increases.\n\nscikit-learn–inspired estimator interface:\nimport autobnn as ab\n\nmodel = ab.operators.Add(\n    bnns=(ab.kernels.PeriodicBNN(width=50),\n          ab.kernels.LinearBNN(width=50),\n          ab.kernels.MaternBNN(width=50)))\n\nestimator = ab.estimators.AutoBnnMapEstimator(\n    model, 'normal_likelihood_logistic_noise', jax.random.PRNGKey(42),\n    periods=[12])\n\nestimator.fit(my_training_data_xs, my_training_data_ys)\nlow, mid, high = estimator.predict_quantiles(my_training_data_xs)\n\n\n\n\n\n    \nConclusion\nAutoBNN provides a powerful and flexible framework for building sophisticated time series prediction models. By combining the strengths of BNNs and GPs with compositional kernels, AutoBNN opens a world of possibilities for understanding and forecasting complex data. We invite the community to try the colab, and leverage this library to innovate and solve real-world challenges. \n\n\n\n    \nAcknowledgements\nAutoBNN was written by Colin Carroll, Thomas Colthurst, Urs Köster and Srinivas Vasudevan. We would like to thank Kevin Murphy, Brian Patton and Feras Saad for their advice and feedback."
  },
  {
    "source": "Google AI Blog",
    "title": "Computer-aided diagnosis for lung cancer screening",
    "link": "http://blog.research.google/2024/03/computer-aided-diagnosis-for-lung.html",
    "pubDate": "2024-03-20T20:54:00.000Z",
    "contentSnippet": "Posted by Atilla Kiraly, Software Engineer, and Rory Pilgrim, Product Manager, Google Research \n\n\n\n\n\nLung cancer is the leading cause of cancer-related deaths globally with 1.8 million deaths reported in 2020. Late diagnosis dramatically reduces the chances of survival. Lung cancer screening via computed tomography (CT), which provides a detailed 3D image of the lungs, has been shown to reduce mortality in high-risk populations by at least 20% by detecting potential signs of cancers earlier. In the US, screening involves annual scans, with some countries or cases recommending more or less frequent scans. \n\n\n\nThe United States Preventive Services Task Force recently expanded lung cancer screening recommendations by roughly 80%, which is expected to increase screening access for women and racial and ethnic minority groups. However, false positives (i.e., incorrectly reporting a potential cancer in a cancer-free patient) can cause anxiety and lead to unnecessary procedures for patients while increasing costs for the healthcare system. Moreover, efficiency in screening a large number of individuals can be challenging depending on healthcare infrastructure and radiologist availability.\n\n\n\n\nAt Google we have previously developed machine learning (ML) models for lung cancer detection, and have evaluated their ability to automatically detect and classify regions that show signs of potential cancer. Performance has been shown to be comparable to that of specialists in detecting possible cancer. While they have achieved high performance, effectively communicating findings in realistic environments is necessary to realize their full potential.\n\n\n\nTo that end, in “Assistive AI in Lung Cancer Screening: A Retrospective Multinational Study in the US and Japan”, published in Radiology AI, we investigate how ML models can effectively communicate findings to radiologists. We also introduce a generalizable user-centric interface to help radiologists leverage such models for lung cancer screening. The system takes CT imaging as input and outputs a cancer suspicion rating using four categories (no suspicion, probably benign, suspicious, highly suspicious) along with the corresponding regions of interest. We evaluate the system’s utility in improving clinician performance through randomized reader studies in both the US and Japan, using the local cancer scoring systems (Lung-RADSs V1.1 and Sendai Score) and image viewers that mimic realistic settings. We found that reader specificity increases with model assistance in both reader studies. To accelerate progress in conducting similar studies with ML models, we have open-sourced code to process CT images and generate images compatible with the picture archiving and communication system (PACS) used by radiologists. \n\n\n\n    \nDeveloping an interface to communicate model results\nalpha-numeric score to indicate the lung cancer risk and follow-up recommendations. When assessing patients, radiologists load the CT in their workstation to read the case, find lung nodules or lesions, and apply set guidelines to determine follow-up decisions. \n\n\n\n\nOur first step was to improve the previously developed ML models through additional training data and architectural improvements, including self-attention. Then, instead of targeting specific guidelines, we experimented with a complementary way of communicating AI results independent of guidelines or their particular versions. Specifically, the system output offers a suspicion rating and localization (regions of interest) for the user to consider in conjunction with their own specific guidelines. The interface produces output images directly associated with the CT study, requiring no changes to the user’s workstation. The radiologist only needs to review a small set of additional images. There is no other change to their system or interaction with the system.\n\n\n\n\n\n\n\n\n\nExample of the assistive lung cancer screening system outputs. Results for the radiologist’s evaluation are visualized on the location of the CT volume where the suspicious lesion is found. The overall suspicion is displayed at the top of the CT images. Circles highlight the suspicious lesions while squares show a rendering of the same lesion from a different perspective, called a sagittal view.\n\nprior work. The models coordinate with each other to first segment the lungs, obtain an overall assessment, locate three suspicious regions, then use the information to assign a suspicion rating to each region. The system was deployed on Google Cloud using a Google Kubernetes Engine (GKE) that pulled the images, ran the ML models, and provided results. This allows scalability and directly connects to servers where the images are stored in DICOM stores.\n\n\n\n\n\n\nOutline of the Google Cloud deployment of the assistive lung cancer screening system and the directional calling flow for the individual components that serve the images and compute results. Images are served to the viewer and to the system using Google Cloud services. The system is run on a Google Kubernetes Engine that pulls the images, processes them, and writes them back into the DICOM store.\n\nReader studies \narea under the ROC curve (AUC) values. These were compared with and without assistance.\n\n\n\n\n\nA multi-case multi-reader study involves each case being reviewed by each reader twice, once with ML system assistance and once without. In this visualization one reader first reviews Set A without assistance (blue) and then with assistance (orange) after a wash-out period. A second reader group follows the opposite path by reading the same set of cases Set A with assistance first. Readers are randomized to these groups to remove the effect of ordering.\n\nspecificity) by an absolute 5–7% compared to when they didn’t use the assistive system. This potentially means that for every 15–20 patients screened, one may be able to avoid unnecessary follow-up procedures, thus reducing their anxiety and the burden on the health care system. This can, in turn, help improve the sustainability of lung cancer screening programs, particularly as more people become eligible for screening. \n\n\n\n\n\nReader specificity increases with ML model assistance in both the US-based and Japan-based reader studies. Specificity values were derived from reader scores from actionable findings (something suspicious was found) versus no actionable findings, compared against the true cancer outcome of the individual.  Under model assistance, readers flagged fewer cancer-negative individuals for follow-up visits. Sensitivity for cancer positive individuals remained the same.\n\nTranslating this into real-world impact through partnership \nDeepHealth, a leading AI-powered health informatics provider; and Apollo Radiology International a leading provider of Radiology services in India to explore paths for incorporating this system into future products. In addition, we are looking to help other researchers studying how best to integrate ML model results into clinical workflows by open sourcing code used for the reader study and incorporating the insights described in this blog. We hope that this will help accelerate medical imaging researchers looking to conduct reader studies for their AI models, and catalyze translational research in the field.  \n\n\n\n\n    \nAcknowledgements\nKey contributors to this project include Corbin Cunningham, Zaid Nabulsi, Ryan Najafi, Jie Yang, Charles Lau, Joseph R. Ledsam, Wenxing Ye, Diego Ardila, Scott M. McKinney, Rory Pilgrim, Hiroaki Saito, Yasuteru Shimamura, Mozziyar Etemadi, Yun Liu, David Melnick, Sunny Jansen, Nadia Harhen, David P. Nadich, Mikhail Fomitchev, Ziyad Helali, Shabir Adeel, Greg S. Corrado, Lily Peng, Daniel Tse, Shravya Shetty, Shruthi Prabhakara, Neeral Beladia, and Krish Eswaran. Thanks to Arnav Agharwal and Andrew Sellergren for their open sourcing support and Vivek Natarajan and Michael D. Howell for their feedback. Sincere appreciation also goes to the radiologists who enabled this work with their image interpretation and annotation efforts throughout the study, and Jonny Wong and Carli Sampson for coordinating the reader studies."
  },
  {
    "source": "Google AI Blog",
    "title": "Using AI to expand global access to reliable flood forecasts",
    "link": "http://blog.research.google/2024/03/using-ai-to-expand-global-access-to.html",
    "pubDate": "2024-03-20T16:06:00.000Z",
    "contentSnippet": "Posted by Yossi Matias, VP Engineering & Research, and Grey Nearing, Research Scientist, Google Research\n\n\n\n\nFloods are the most common natural disaster, and are responsible for roughly $50 billion in annual financial damages worldwide. The rate of flood-related disasters has more than doubled since the year 2000 partly due to climate change. Nearly 1.5 billion people, making up 19% of the world’s population, are exposed to substantial risks from severe flood events. Upgrading early warning systems to make accurate and timely information accessible to these populations can save thousands of lives per year. \n\n\n\nDriven by the potential impact of reliable flood forecasting on people’s lives globally, we started our flood forecasting effort in 2017. Through this multi-year journey, we advanced research over the years hand-in-hand with building a real-time operational flood forecasting system that provides alerts on Google Search, Maps, Android notifications and through the Flood Hub. However, in order to scale globally, especially in places where accurate local data is not available, more research advances were required.\n\n\n\nIn “Global prediction of extreme floods in ungauged watersheds”, published in Nature, we demonstrate how machine learning (ML) technologies can significantly improve global-scale flood forecasting relative to the current state-of-the-art for countries where flood-related data is scarce. With these AI-based technologies we extended the reliability of currently-available global nowcasts, on average, from zero to five days, and improved forecasts across regions in Africa and Asia to be similar to what are currently available in Europe. The evaluation of the models was conducted in collaboration with the European Center for Medium Range Weather Forecasting (ECMWF).\n\n\n\nThese technologies also enable Flood Hub to provide real-time river forecasts up to seven days in advance, covering river reaches across over 80 countries. This information can be used by people, communities, governments and international organizations to take anticipatory action to help protect vulnerable populations.\n\n\n\n\nFlood forecasting at Google \nlaunched a pilot early warning system in the Ganges-Brahmaputra river basin in India, with the hypothesis that ML could help address the challenging problem of reliable flood forecasting at scale. The pilot was further expanded the following year via the combination of an inundation model, real-time water level measurements, the creation of an elevation map and hydrologic modeling.\n\n\n\nIn collaboration with academics, and, in particular, with the JKU Institute for Machine Learning we explored ML-based hydrologic models, showing that LSTM-based models could produce more accurate simulations than traditional conceptual and physics-based hydrology models. This research led to flood forecasting improvements that enabled the expansion of our forecasting coverage to include all of India and Bangladesh. We also worked with researchers at Yale University to test technological interventions that increase the reach and impact of flood warnings.\n\n\n\nOur hydrological models predict river floods by processing publicly available weather data like precipitation and physical watershed information. Such models must be calibrated to long data records from streamflow gauging stations in individual rivers. A low percentage of global river watersheds (basins) have streamflow gauges, which are expensive but necessary to supply relevant data, and it’s challenging for hydrological simulation and forecasting to provide predictions in basins that lack this infrastructure. Lower gross domestic product (GDP) is correlated with increased vulnerability to flood risks, and there is an inverse correlation between national GDP and the amount of publicly available data in a country. ML helps to address this problem by allowing a single model to be trained on all available river data and to be applied to ungauged basins where no data are available. In this way, models can be trained globally, and can make predictions for any river location.\n\n\n\n\n\nThere is an inverse (log-log) correlation between the amount of publicly available streamflow data in a country and national GDP. Streamflow data from the Global Runoff Data Center.\n\nestimate uncertainty in river forecasts and showed how ML river forecast models synthesize information from multiple data sources. They demonstrated that these models can simulate extreme events reliably, even when those events are not part of the training data. In an effort to contribute to open science, in 2023 we open-sourced a community-driven dataset for large-sample hydrology in Nature Scientific Data. \n\n\n\n\n    \nThe river forecast model\nLSTMs perform well on the task of river forecasting.\n\n\n\n\n\nA diagram of the LSTM, which is a neural network that operates sequentially in time. An accessible primer can be found here.\n\nmixture density networks to produce a probabilistic forecast (i.e., predicted parameters of a probability distribution over streamflow). Specifically, the model predicts the parameters of a mixture of heavy-tailed probability density functions, called asymmetric Laplacian distributions, at each forecast time step. The result is a mixture density function, called a Countable Mixture of Asymmetric Laplacians (CMAL) distribution, which represents a probabilistic prediction of the volumetric flow rate in a particular river at a particular time. \n\n\n\n\n\nLSTM-based river forecast model architecture. Two LSTMs are applied in sequence, one ingesting historical weather data and one ingesting forecasted weather data. The model outputs are the parameters of a probability distribution over streamflow at each forecasted timestep.\n\nInput and training data\nStatic watershed attributes representing geographical and geophysical variables: From the HydroATLAS project, including data like long-term climate indexes (precipitation, temperature, snow fractions), land cover, and anthropogenic attributes (e.g., a nighttime lights index as a proxy for human development). \n\n\nHistorical meteorological time-series data: Used to spin up the model for one year prior to the issue time of a forecast. The data comes from NASA IMERG, NOAA  CPC  Global Unified Gauge-Based Analysis of Daily Precipitation, and the ECMWF ERA5-land reanalysis. Variables include daily total precipitation, air temperature, solar and thermal radiation, snowfall, and surface pressure. \n\n\nForecasted meteorological time series over a seven-day forecast horizon: Used as input for the forecast LSTM. These data are the same meteorological variables listed above, and come from the ECMWF HRES atmospheric model.\n\n\n\n\nTraining data are daily streamflow values from the Global Runoff Data Center over the time period 1980 - 2023. A single streamflow forecast model is trained using data from 5,680 diverse watershed streamflow gauges (shown below) to improve accuracy.\n\n\n\n\n\nLocation of 5,680 streamflow gauges that supply training data for the river forecast model from the Global Runoff Data Center.\n\n  \nImproving on the current state-of-the-art\nGloFAS version 4, the current state-of-the-art global flood forecasting system. These experiments showed that ML can provide accurate warnings earlier and over larger and more impactful events. \n\n\n\nThe figure below shows the distribution of F1 scores when predicting different severity events at river locations around the world, with plus or minus 1 day accuracy. F1 scores are an average of precision and recall and event severity is measured by return period. For example, a 2-year return period event is a volume of streamflow that is expected to be exceeded on average once every two years. Our model achieves reliability scores at up to 4-day or 5-day lead times that are similar to or better, on average, than the reliability of GloFAS nowcasts (0-day lead time). \n\n\n\n\n\nDistributions of F1 scores over 2-year return period events in 2,092 watersheds globally during the time period 2014-2023 from GloFAS (blue) and our model (orange) at different lead times. On average, our model is statistically as accurate as GloFAS nowcasts (0–day lead time) up to 5 days in advance over 2-year (shown) and 1-year, 5-year, and 10-year events (not shown).\n\npaper for more information.\n\n\n\n    \nLooking into the future\nAdaptation and Resilience efforts and reflects Google's commitment to address climate change while helping global communities become more resilient. We believe that AI and ML will continue to play a critical role in helping advance science and research towards climate action.\n\n\n\nWe actively collaborate with several international aid organizations (e.g., the Centre for Humanitarian Data and the Red Cross) to provide actionable flood forecasts. Additionally, in an ongoing collaboration with the World Meteorological Organization (WMO) to support early warning systems for climate hazards, we are conducting a study to help understand how AI can help address real-world challenges faced by national flood forecasting agencies. \n\n\n\nWhile the work presented here demonstrates a significant step forward in flood forecasting, future work  is needed to further expand flood forecasting coverage to more locations globally and other types of flood-related events and disasters, including flash floods and urban floods. We are looking forward to continuing collaborations with our partners in the academic and expert communities, local governments and the industry to reach these goals."
  },
  {
    "source": "Google AI Blog",
    "title": "ScreenAI: A visual language model for UI and visually-situated language understanding",
    "link": "http://blog.research.google/2024/03/screenai-visual-language-model-for-ui.html",
    "pubDate": "2024-03-19T20:15:00.000Z",
    "contentSnippet": "Posted by Srinivas Sunkara and Gilles Baechler, Software Engineers, Google Research\n\n\n\n\n\nScreen user interfaces (UIs) and infographics, such as charts, diagrams and tables, play important roles in human communication and human-machine interaction as they facilitate rich and interactive user experiences. UIs and infographics share similar design principles and visual language (e.g., icons and layouts), that offer an opportunity to build a single model that can understand, reason, and interact with these interfaces. However, because of their complexity and varied presentation formats, infographics and UIs present a unique modeling challenge.\n\n\n\nTo that end, we introduce “ScreenAI: A Vision-Language Model for UI and Infographics Understanding”. ScreenAI improves upon the PaLI architecture with the flexible patching strategy from pix2struct. We train ScreenAI on a unique mixture of datasets and tasks, including a novel Screen Annotation task that requires the model to identify UI element information (i.e., type, location and description) on a screen. These text annotations provide large language models (LLMs) with screen descriptions, enabling them to automatically generate question-answering (QA), UI navigation, and summarization training datasets at scale. At only 5B parameters, ScreenAI achieves state-of-the-art results on UI- and infographic-based tasks (WebSRC and MoTIF), and best-in-class performance on Chart QA, DocVQA, and InfographicVQA compared to models of similar size. We are also releasing three new datasets: Screen Annotation to evaluate the layout understanding capability of the model, as well as ScreenQA Short and Complex ScreenQA for a more comprehensive evaluation of its QA capability. \n\n\n\n    \nScreenAI\nPaLI, composed of a multimodal encoder block and an autoregressive decoder. The PaLI encoder uses a vision transformer (ViT) that creates image embeddings and a multimodal encoder that takes the concatenation of the image and text embeddings as input. This flexible architecture allows ScreenAI to solve vision tasks that can be recast as text+image-to-text problems. \n\n\n\nOn top of the PaLI architecture, we employ a flexible patching strategy introduced in pix2struct. Instead of using a fixed-grid pattern, the grid dimensions are selected such that they preserve the native aspect ratio of the input image. This enables ScreenAI to work well across images of various aspect ratios. \n\n\n\nThe ScreenAI model is trained in two stages: a pre-training stage followed by a fine-tuning stage. First, self-supervised learning is applied to automatically generate data labels, which are then used to train ViT and the language model. ViT is frozen during the fine-tuning stage, where most data used is manually labeled by human raters. \n\n\n\n\n\nScreenAI model architecture.\n\nData generation\npublicly accessible web pages and following the programmatic exploration approach used for the RICO dataset for mobile apps. We then apply a layout annotator, based on the DETR model, that identifies and labels a wide range of UI elements (e.g., image, pictogram, button, text) and their spatial relationships. Pictograms undergo further analysis using an icon classifier capable of distinguishing 77 different icon types. This detailed classification is essential for interpreting the subtle information conveyed through icons. For icons that are not covered by the classifier, and for infographics and images, we use the PaLI image captioning model to generate descriptive captions that provide contextual information. We also apply an optical character recognition (OCR) engine to extract and annotate textual content on screen. We combine the OCR text with the previous annotations to create a detailed description of each screen.\n\n\n\n\n\nA mobile app screenshot with generated annotations that include UI elements and their descriptions, e.g., TEXT elements also contain the text content from OCR, IMAGE elements contain image captions, LIST_ITEMs contain all their child elements.\n\nLLM-based data generation\nPaLM 2 to generate input-output pairs in a two-step process. First, screen annotations are generated using the technique outlined above, then we craft a prompt around this schema for the LLM to create synthetic data. This process requires prompt engineering and iterative refinement to find an effective prompt. We assess the generated data's quality through human validation against a quality threshold. \n\n\n\n\nYou only speak JSON. Do not write text that isn’t JSON.\nYou are given the following mobile screenshot, described in words. Can you generate 5 questions regarding the content of the screenshot as well as the corresponding short answers to them? \n\nThe answer should be as short as possible, containing only the necessary information. Your answer should be structured as follows:\nquestions: [\n{{question: the question,\n    answer: the answer\n}},\n ...\n]\n\n{THE SCREEN SCHEMA}\n\n\nA sample prompt for QA data generation.\n\nQuestion answering: The model is asked to answer questions regarding the content of the screenshots, e.g., “When does the restaurant open?”\n\n\nScreen navigation: The model is asked to convert a natural language utterance into an executable action on a screen, e.g., “Click the search button.”\n\n\nScreen summarization: The model is asked to summarize the screen content in one or two sentences. \n\n\n\n\n\n\nBlock diagram of our workflow for generating data for QA, summarization and navigation tasks using existing ScreenAI models and LLMs. Each task uses a custom prompt to emphasize desired aspects, like questions related to counting, involving reasoning, etc.\n\n\n\nLLM-generated data. Examples for screen QA, navigation and summarization. For navigation, the action bounding box is displayed in red on the screenshot.\n\nExperiments and results\nChartQA, DocVQA, Multi page DocVQA, InfographicVQA, OCR VQA, Web SRC and ScreenQA. For navigation, datasets used include Referring Expressions, MoTIF, Mug, and Android in the Wild. Finally, we use Screen2Words for screen summarization and Widget Captioning for describing specific UI elements. Along with the fine-tuning datasets, we  evaluate the fine-tuned ScreenAI model using three novel benchmarks:\n\n\n\n\nScreen Annotation: Enables the evaluation model layout annotations and spatial understanding capabilities.\n\n\nScreenQA Short: A variation of ScreenQA, where its ground truth answers have been shortened to contain only the relevant information that better aligns with other QA tasks.\n\n\nComplex ScreenQA: Complements ScreenQA Short with more difficult questions (counting, arithmetic, comparison, and non-answerable questions) and contains screens with various aspect ratios.\n\n\n\n\nThe fine-tuned ScreenAI model achieves state-of-the-art results on various UI and infographic-based tasks (WebSRC and MoTIF) and best-in-class performance on Chart QA, DocVQA, and InfographicVQA compared to models of similar size. ScreenAI achieves competitive performance on Screen2Words and OCR-VQA. Additionally, we report results on the new benchmark datasets introduced to serve as a baseline for further research.\n\n\n\n\nComparing model performance of ScreenAI with state-of-the-art (SOTA) models of similar size.\n\n\n\nModel performance increases with size, and the performance has not saturated even at the largest size of 5B params.\n\nConclusion\nAcknowledgements\nThis project is the result of joint work with Maria Wang, Fedir Zubach, Hassan Mansoor, Vincent Etter, Victor Carbune, Jason Lin, Jindong Chen and Abhanshu Sharma. We thank Fangyu Liu, Xi Chen, Efi Kokiopoulou, Jesse Berent, Gabriel Barcik, Lukas Zilka, Oriana Riva, Gang Li,Yang Li, Radu Soricut, and Tania Bedrax-Weiss for their insightful feedback and discussions, along with Rahul Aralikatte, Hao Cheng and Daniel Kim for their support in data preparation. We also thank Jay Yagnik, Blaise Aguera y Arcas, Ewa Dominowska, David Petrou, and Matt Sharifi for their leadership, vision and support. We are very grateful toTom Small for helping us create the animation in this post."
  },
  {
    "source": "Google AI Blog",
    "title": "SCIN: A new resource for representative dermatology images",
    "link": "http://blog.research.google/2024/03/scin-new-resource-for-representative.html",
    "pubDate": "2024-03-19T15:00:00.000Z",
    "contentSnippet": "Posted by Pooja Rao, Research Scientist, Google Research\n\n\n\n\nHealth datasets play a crucial role in research and medical education, but it can be challenging to create a dataset that represents the real world. For example, dermatology conditions are diverse in their appearance and severity and manifest differently across skin tones. Yet, existing dermatology image datasets often lack representation of everyday conditions (like rashes, allergies and infections) and skew towards lighter skin tones. Furthermore, race and ethnicity information is frequently missing, hindering our ability to assess disparities or create solutions.\n\n\n\n\n\nTo address these limitations, we are releasing the Skin Condition Image Network (SCIN) dataset in collaboration with physicians at Stanford Medicine. We designed SCIN to reflect the broad range of concerns that people search for online, supplementing the types of conditions typically found in clinical datasets. It contains images across various skin tones and body parts, helping to ensure that future AI tools work effectively for all. We've made the SCIN dataset freely available as an open-access resource for researchers, educators, and developers, and have taken careful steps to protect contributor privacy.   \n\n\n\n\n\n\nExample set of images and metadata from the SCIN dataset.\n\nDataset composition\ntanning propensity (self-reported Fitzpatrick Skin Type, i.e., sFST), and to describe the texture, duration and symptoms related to their concern.\n\n\nOne to three dermatologists labeled each contribution with up to five dermatology conditions, along with a confidence score for each label. The SCIN dataset contains these individual labels, as well as an aggregated and weighted differential diagnosis derived from them that could be useful for model testing or training. These labels were assigned retrospectively and are not equivalent to a clinical diagnosis, but they allow us to compare the distribution of dermatology conditions in the SCIN dataset with existing datasets.\n\n\n\n\n\n\n\nThe SCIN dataset contains largely allergic, inflammatory and infectious conditions while datasets from clinical sources focus on benign and malignant neoplasms.\n\nMonk Skin Tone (eMST) for the images. This allowed comparison of the skin condition and skin type distributions to those in existing dermatology datasets. Although we did not selectively target any skin types or skin tones, the SCIN dataset has a balanced Fitzpatrick skin type distribution (with more of Types 3, 4, 5, and 6) compared to similar datasets from clinical sources. \n\n\n\n\n\n\n\nSelf-reported and dermatologist-estimated Fitzpatrick Skin Type distribution in the SCIN dataset compared with existing un-enriched dermatology datasets (Fitzpatrick17k, PH², SKINL2, and PAD-UFES-20).\n\nFitzpatrick Skin Type scale was originally developed as a photo-typing scale to measure the response of skin types to UV radiation, and it is widely used in dermatology research. The Monk Skin Tone scale is a newer 10-shade scale that measures skin tone rather than skin phototype, capturing more nuanced differences between the darker skin tones. While neither scale was intended for retrospective estimation using images, the inclusion of these labels is intended to enable future research into skin type and tone representation in dermatology. For example, the SCIN dataset provides an initial benchmark for the distribution of these skin types and tones in the US population.\n\n\nThe SCIN dataset has a high representation of women and younger individuals, likely reflecting a combination of factors. These could include differences in skin condition incidence, propensity to seek health information online, and variations in willingness to contribute to research across demographics.\n\n\n\n\n\n    \nCrowdsourcing method\nresearch paper co-authored with investigators at Stanford Medicine. This approach empowers individuals to play an active role in healthcare research. It allows us to reach people at earlier stages of their health concerns, potentially before they seek formal care. Crucially, this method uses advertisements on web search result pages — the starting point for many people’s health journey — to connect with participants. \n\n\nOur results demonstrate that crowdsourcing can yield a high-quality dataset with a low spam rate. Over 97.5% of contributions were genuine images of skin conditions. After performing further filtering steps to exclude images that were out of scope for the SCIN dataset and to remove duplicates, we were able to release nearly 90% of the contributions received over the 8-month study period. Most images were sharp and well-exposed. Approximately half of the contributions include self-reported demographics, and 80% contain self-reported information relating to the skin condition, such as texture, duration, or other symptoms. We found that dermatologists’ ability to retrospectively assign a differential diagnosis depended more on the availability of self-reported information than on image quality.\n\n\n\n\n\n\n\nDermatologist confidence in their labels (scale from 1-5) depended on the availability of self-reported demographic and symptom information.\n\nData Use License prohibits attempts to re-identify contributors.\n\n\nWe hope the SCIN dataset will be a helpful resource for those working to advance inclusive dermatology research, education, and AI tool development. By demonstrating an alternative to traditional dataset creation methods, SCIN paves the way for more representative datasets in areas where self-reported data or retrospective labeling is feasible. \n\n\n\n\n\n    \nAcknowledgements\nWe are grateful to all our co-authors Abbi Ward, Jimmy Li, Julie Wang, Sriram Lakshminarasimhan, Ashley Carrick, Bilson Campana, Jay Hartford, Pradeep Kumar S, Tiya Tiyasirisokchai, Sunny Virmani, Renee Wong, Yossi Matias, Greg S. Corrado, Dale R. Webster, Dawn Siegel (Stanford Medicine), Steven Lin (Stanford Medicine), Justin Ko (Stanford Medicine), Alan Karthikesalingam and Christopher Semturs. We also thank Yetunde Ibitoye, Sami Lachgar, Lisa Lehmann, Javier Perez, Margaret Ann Smith (Stanford Medicine), Rachelle Sico, Amit Talreja, Annisah Um’rani and Wayne Westerlind for their essential contributions to this work. Finally, we are grateful to Heather Cole-Lewis, Naama Hammel, Ivor Horn, Michael Howell, Yun Liu, and Eric Teasley for their insightful comments on the study design and manuscript."
  },
  {
    "source": "Google AI Blog",
    "title": "MELON: Reconstructing 3D objects from images with unknown poses",
    "link": "http://blog.research.google/2024/03/melon-reconstructing-3d-objects-from.html",
    "pubDate": "2024-03-18T18:41:00.000Z",
    "contentSnippet": "Posted by Mark Matthews, Senior Software Engineer, and Dmitry Lagun, Research Scientist, Google Research\n\n\n\n\n\nA person's prior experience and understanding of the world generally enables them to easily infer what an object looks like in whole, even if only looking at a few 2D pictures of it. Yet the capacity for a computer to reconstruct the shape of an object in 3D given only a few images has remained a difficult algorithmic problem for years. This fundamental computer vision task has applications ranging from the creation of e-commerce 3D models to autonomous vehicle navigation. \n\n\n\nA key part of the problem is how to determine the exact positions from which images were taken, known as pose inference. If camera poses are known, a range of successful techniques — such as neural radiance fields (NeRF) or 3D Gaussian Splatting — can reconstruct an object in 3D. But if these poses are not available, then we face a difficult “chicken and egg” problem where we could determine the poses if we knew the 3D object, but we can’t reconstruct the 3D object until we know the camera poses. The problem is made harder by pseudo-symmetries — i.e., many objects look similar when viewed from different angles. For example, square objects like a chair tend to look similar every 90° rotation. Pseudo-symmetries of an object can be revealed by rendering it on a turntable from various angles and plotting its photometric self-similarity map. \n\n\n\n\nSelf-Similarity map of a toy truck model. Left: The model is rendered on a turntable from various azimuthal angles, θ. Right: The average L2 RGB similarity of a rendering from θ with that of θ*. The pseudo-similarities are indicated by the dashed red lines.\n\nill-posed, with naïve approaches often converging to local minima. In practice, such an approach might mistake the back view as the front view of an object, because they share a similar silhouette. Previous techniques (such as BARF or SAMURAI) side-step this problem by relying on an initial pose estimate that starts close to the global minima. But how can we approach this if those aren’t available?\n\n\n\nMethods, such as GNeRF and VMRF leverage generative adversarial networks (GANs) to overcome the problem. These techniques have the ability to artificially “amplify” a limited number of training views, aiding reconstruction. GAN techniques, however, often have complex, sometimes unstable, training processes, making robust and reliable convergence difficult to achieve in practice. A range of other successful methods, such as SparsePose or RUST, can infer poses from a limited number views, but require pre-training on a large dataset of posed images, which aren’t always available, and can suffer from “domain-gap” issues when inferring poses for different types of images.\n\n\n\nIn “MELON: NeRF with Unposed Images in SO(3)”, spotlighted at 3DV 2024, we present a technique that can determine object-centric camera poses entirely from scratch while reconstructing the object in 3D. MELON (Modulo Equivalent Latent Optimization of NeRF) is one of the first techniques that can do this without initial pose camera estimates, complex training schemes or pre-training on labeled data. MELON is a relatively simple technique that can easily be integrated into existing NeRF methods. We demonstrate that MELON can reconstruct a NeRF from unposed images with state-of-the-art accuracy while requiring as few as 4–6 images of an object. \n\n\n\n\n    \nMELON\nconvolutional neural network (CNN) encoder that regresses camera poses from training images. We pass a downscaled training image to a four layer CNN that infers the camera pose. This CNN is initialized from noise and requires no pre-training. Its capacity is so small that it forces similar looking images to similar poses, providing an implicit regularization greatly aiding convergence.\n\n\n\nThe second technique is a modulo loss that simultaneously considers pseudo symmetries of an object. We render the object from a fixed set of viewpoints for each training image, backpropagating the loss only through the view that best fits the training image. This effectively considers the plausibility of multiple views for each image. In practice, we find N=2 views (viewing an object from the other side) is all that’s required in most cases, but sometimes get better results with N=4 for square objects.\n\n\n\nThese two techniques are integrated into standard NeRF training, except that instead of fixed camera poses, poses are inferred by the CNN and duplicated by the modulo loss. Photometric gradients back-propagate through the best-fitting cameras into the CNN. We observe that cameras generally converge quickly to globally optimal poses (see animation below). After training of the neural field, MELON can synthesize novel views using standard NeRF rendering methods.\n\n\n\nWe simplify the problem by using the NeRF-Synthetic dataset, a popular benchmark for NeRF research and common in the pose-inference literature. This synthetic dataset has cameras at precisely fixed distances and a consistent “up” orientation, requiring us to infer only the polar coordinates of the camera. This is the same as an object at the center of a globe with a camera always pointing at it, moving along the surface. We then only need the latitude and longitude (2 degrees of freedom) to specify the camera pose.\n\n\n\n\n\nMELON uses a dynamically trained lightweight CNN encoder that predicts a pose for each image. Predicted poses are replicated by the modulo loss, which only penalizes the smallest L2 distance from the ground truth color. At evaluation time, the neural field can be used to generate novel views.\n\nResults\npeak signal-to-noise ratio (PSNR) against held out test views. We see that MELON quickly converges to the approximate poses of most cameras within the first 1,000 steps of training, and achieves a competitive PSNR of 27.5 dB after 50k steps. \n\n\n\n\n\nConvergence of MELON on a toy truck model during optimization. Left: Rendering of the NeRF. Right: Polar plot of predicted (blue x), and ground truth (red dot) cameras.\n\n\n\nReconstruction quality comparison between ground-truth (GT) and MELON on NeRF-Synthetic scenes after 100k training steps.\n\nNoisy images\nnovel view synthesis from extremely noisy, unposed images. We add varying amounts, σ, of white Gaussian noise to the training images. For example, the object in σ=1.0 below is impossible to make out, yet MELON can determine the pose and generate novel views of the object. \n\n\n\n\n\n\nNovel view synthesis from noisy unposed 128×128 images. Top: Example of noise level present in training views. Bottom: Reconstructed model from noisy training views and mean angular pose error.\n\nRawNeRF have demonstrated NeRF’s excellent de-noising capabilities with known camera poses. The fact that MELON works for noisy images of unknown camera poses so robustly was unexpected. \n\n\n\n    \nConclusion\npaper and MELON site to learn more.\n\n\n\n\n    \nAcknowledgements\nWe would like to thank our paper co-authors Axel Levy, Matan Sela, and Gordon Wetzstein, as well as Florian Schroff and Hartwig Adam for continuous help in building this technology. We also thank Matthew Brown, Ricardo Martin-Brualla and Frederic Poitevin for their helpful feedback on the paper draft. We also acknowledge the use of the computational resources at the SLAC Shared Scientific Data Facility (SDF)."
  },
  {
    "source": "Google AI Blog",
    "title": "HEAL: A framework for health equity assessment of machine learning performance",
    "link": "http://blog.research.google/2024/03/heal-framework-for-health-equity.html",
    "pubDate": "2024-03-15T18:22:00.000Z",
    "contentSnippet": "Posted by Mike Schaekermann, Research Scientist, Google Research, and Ivor Horn, Chief Health Equity Officer & Director, Google Core\n\n\n\n\nHealth equity is a major societal concern worldwide with disparities having many causes. These sources include limitations in access to healthcare, differences in clinical treatment, and even fundamental differences in the diagnostic technology. In dermatology for example, skin cancer outcomes are worse for populations such as minorities, those with lower socioeconomic status, or individuals with limited healthcare access. While there is great promise in recent advances in machine learning (ML) and artificial intelligence (AI) to help improve healthcare, this transition from research to bedside must be accompanied by a careful understanding of whether and how they impact health equity.\n\n \n\n\nHealth equity is defined by public health organizations as fairness of opportunity for everyone to be as healthy as possible. Importantly, equity may be different from equality. For example, people with greater barriers to improving their health may require more or different effort to experience this fair opportunity. Similarly, equity is not fairness as defined in the AI for healthcare literature. Whereas AI fairness often strives for equal performance of the AI technology across different patient populations, this does not center the goal of prioritizing performance with respect to pre-existing health disparities.\n\n\n\n\nHealth equity considerations. An intervention (e.g., an ML-based tool, indicated in dark blue) promotes health equity if it helps reduce existing disparities in health outcomes (indicated in lighter blue).\n\nHealth Equity Assessment of machine Learning performance (HEAL): a framework and dermatology AI model case study”, published in The Lancet eClinicalMedicine, we propose a methodology to quantitatively assess whether ML-based health technologies perform equitably. In other words, does the ML model perform well for those with the worst health outcomes for the condition(s) the model is meant to address? This goal anchors on the principle that health equity should prioritize and measure model performance with respect to disparate health outcomes, which may be due to a number of factors that include structural inequities (e.g., demographic, social, cultural, political, economic, environmental and geographic).\n\n \n\nThe health equity framework (HEAL)\n\n\nFramework for Health Equity Assessment of machine Learning performance (HEAL). Our guiding principle is to avoid exacerbating health inequities, and these steps help us identify disparities and assess for inequitable model performance to move towards better outcomes for all.\n\n \n\nCase study on a dermatology model\nprior work. This example dermatology model was trained to classify 288 skin conditions using a development dataset of 29k cases. The input to the model consists of three photos of a skin concern along with demographic information and a brief structured medical history. The output consists of a ranked list of possible matching skin conditions. \n\n\nUsing the HEAL framework, we evaluated this model by assessing whether it prioritized performance with respect to pre-existing health outcomes. The model was designed to predict possible dermatologic conditions (from a list of hundreds) based on photos of a skin concern and patient metadata. Evaluation of the model is done using a top-3 agreement metric, which quantifies how often the top 3 output conditions match the most likely condition as suggested by a dermatologist panel. The HEAL metric is computed via the anticorrelation of this top-3 agreement with health outcome rankings. \n\n\nWe used a dataset of 5,420 teledermatology cases, enriched for diversity in age, sex and race/ethnicity, to retrospectively evaluate the model’s HEAL metric. The dataset consisted of “store-and-forward” cases from patients of 20 years or older from primary care providers in the USA and skin cancer clinics in Australia. Based on a review of the literature, we decided to explore race/ethnicity, sex and age as potential factors of inequity, and used sampling techniques to ensure that our evaluation dataset had sufficient representation of all race/ethnicity, sex and age groups. To quantify pre-existing health outcomes for each subgroup we relied on measurements from public databases endorsed by the World Health Organization, such as Years of Life Lost (YLLs) and Disability-Adjusted Life Years (DALYs; years of life lost plus years lived with disability).\n\n\n\n\nHEAL metric for all dermatologic conditions across race/ethnicity subpopulations, including health outcomes (YLLs per 100,000), model performance (top-3 agreement), and rankings for health outcomes and tool performance.\n(* Higher is better; measures the likelihood the model performs equitably with respect to the axes in this table.)\n\n\n\nHEAL metric for all dermatologic conditions across sexes, including health outcomes (DALYs per 100,000), model performance (top-3 agreement), and rankings for health outcomes and tool performance. (* As above.)\n\n\n\nHEAL metrics for all cancer and non-cancer dermatologic conditions across age groups, including health outcomes (DALYs per 100,000), model performance (top-3 agreement), and rankings for health outcomes and tool performance. (* As above.)\n\n \n\nPutting things in context\nPareto condition (discussed further in the paper), which restricts model changes so that outcomes for each subpopulation are either unchanged or improved compared to the status quo, and performance does not worsen for any subpopulation.\n\n\nThe HEAL framework, in its current form, assesses the likelihood that an ML-based model prioritizes performance for subpopulations with respect to pre-existing health disparities for specific subpopulations. This differs from the goal of understanding whether ML will reduce disparities in outcomes across subpopulations in reality. Specifically, modeling improvements in outcomes requires a causal understanding of steps in the care journey that happen both before and after use of any given model. Future research is needed to address this gap.\n\n \n\nConclusion\n \n\nAcknowledgements\nThe research described here is joint work across many teams at Google. We are grateful to all our co-authors: Terry Spitz, Malcolm Pyles, Heather Cole-Lewis, Ellery Wulczyn, Stephen R. Pfohl, Donald Martin, Jr., Ronnachai Jaroensri, Geoff Keeling, Yuan Liu, Stephanie Farquhar, Qinghan Xue, Jenna Lester, Cían Hughes, Patricia Strachan, Fraser Tan, Peggy Bui, Craig H. Mermel, Lily H. Peng, Yossi Matias, Greg S. Corrado, Dale R. Webster, Sunny Virmani, Christopher Semturs, Yun Liu, and Po-Hsuan Cameron Chen. We also thank Lauren Winer, Sami Lachgar, Ting-An Lin, Aaron Loh, Morgan Du, Jenny Rizk, Renee Wong, Ashley Carrick, Preeti Singh, Annisah Um'rani, Jessica Schrouff, Alexander Brown, and Anna Iurchenko for their support of this project."
  },
  {
    "source": "Google AI Blog",
    "title": "Cappy: Outperforming and boosting large multi-task language models with a small scorer",
    "link": "http://blog.research.google/2024/03/cappy-outperforming-and-boosting-large.html",
    "pubDate": "2024-03-14T19:38:00.000Z",
    "contentSnippet": "Posted by Yun Zhu and Lijuan Liu, Software Engineers, Google Research\n\n\n\n\n\nLarge language model (LLM) advancements have led to a new paradigm that unifies various natural language processing (NLP) tasks within an instruction-following framework. This paradigm is exemplified by recent multi-task LLMs, such as T0, FLAN, and OPT-IML. First, multi-task data is gathered with each task following a task-specific template, where each labeled example is converted into an instruction (e.g., \"Put the concepts together to form a sentence: ski, mountain, skier”) paired with a corresponding response (e.g., \"Skier skis down the mountain\"). These instruction-response pairs are used to train the LLM, resulting in a conditional generation model that takes an instruction as input and generates a response. Moreover, multi-task LLMs have exhibited remarkable task-wise generalization capabilities as they can address unseen tasks by understanding and solving brand-new instructions.\n\n\n\n\n\nThe demonstration of the instruction-following pre-training of multi-task LLMs, e.g., FLAN. Pre-training tasks under this paradigm improves the performance for unseen tasks.\n\nFLAN-11B, T0-11B and OPT-IML-175B). As a result, operating such sizable models poses significant challenges because they demand considerable computational power and impose substantial requirements on the memory capacities of GPUs and TPUs, making their training and inference expensive and inefficient. Extensive storage is required to maintain a unique LLM copy for each downstream task. Moreover, the most powerful multi-task LLMs (e.g., FLAN-PaLM-540B) are closed-sourced, making them impossible to be adapted. However, in practical applications, harnessing a single multi-task LLM to manage all conceivable tasks in a zero-shot manner remains difficult, particularly when dealing with complex tasks, personalized tasks and those that cannot be succinctly defined using instructions. On the other hand, the size of downstream training data is usually insufficient to train a model well without incorporating rich prior knowledge. Hence, it is long desired to adapt LLMs with downstream supervision while bypassing storage, memory, and access issues. \n\n\n\nCertain parameter-efficient tuning strategies, including prompt tuning and adapters, substantially diminish storage requirements, but they still perform back-propagation through LLM parameters during the tuning process, thereby keeping their memory demands high. Additionally, some in-context learning techniques circumvent parameter tuning by integrating a limited number of supervised examples into the instruction. However, these techniques are constrained by the model's maximum input length, which permits only a few samples to guide task resolution.\n\n\n\nIn “Cappy: Outperforming and Boosting Large Multi-Task LMs with a Small Scorer”, presented at NeurIPS 2023, we propose a novel approach that enhances the performance and efficiency of multi-task LLMs. We introduce a lightweight pre-trained scorer, Cappy, based on continual pre-training on top of RoBERTa with merely 360 million parameters. Cappy takes in an instruction and a candidate response as input, and produces a score between 0 and 1, indicating an estimated correctness of the response with respect to the instruction. Cappy functions either independently on classification tasks or serves as an auxiliary component for LLMs, boosting their performance. Moreover, Cappy efficiently enables downstream supervision without requiring any finetuning, which avoids the need for back-propagation through LLM parameters and reduces memory requirements. Finally, adaptation with Cappy doesn’t require access to LLM parameters as it is compatible with closed-source multi-task LLMs, such as those only accessible via WebAPIs.\n\n\n\n\n\nCappy takes an instruction and response pair as input and outputs a score ranging from 0 to 1, indicating an estimation of the correctness of the response with respect to the instruction.\n\nPre-training\nPromptSource that were used to train T0. This collection encompasses a wide range of task types, such as question answering, sentiment analysis, and summarization. Each dataset is associated with one or more templates that convert each instance from the original datasets into an instruction paired with its ground truth response.\n\n\n\nCappy's regression modeling requires each pre-training data instance to include an instruction-response pair along with a correctness annotation for the response, so we produce a dataset with correctness annotations that range from 0 to 1. For every instance within a generation task, we leverage an existing multi-task LLM to generate multiple responses by sampling, conditioned on the given instruction. Subsequently, we assign an annotation to the pair formed by the instruction and every response, using the similarity between the response and the ground truth response of the instance. Specifically, we employ Rouge-L, a commonly-used metric for measuring overall multi-task performance that has demonstrated a strong alignment with human evaluation, to calculate this similarity as a form of weak supervision.\n\n\n\nAs a result, we obtain an effective regression dataset of 160 million instances paired with correctness score annotations. The final Cappy model is the result of continuous pre-training using the regression dataset on top of the RoBERTa model. The pre-training of Cappy is conducted on Google's TPU-v4, with RedCoast, a lightweight toolkit for automating distributed training.\n\n\n\n\n\nData augmentation with a multi-task LLM to construct a weakly supervised regression dataset for Cappy’s pre-training and fine-tuning.\n\nApplying Cappy\nAdapting multi-task LLMs with Cappy \n\n\nDownstream adaptation comparison between Cappy and approaches that rely on an LLM’s parameters, such as fine-tuning and prompt tuning. Cappy’s application enhances multi-task LLMs.\n\nResults\nPromptSource. We demonstrate that Cappy, with 360M parameters, outperforms OPT-175B and OPT-IML-30B, and matches the accuracy of  the best existing multi-task LLMs (T0-11B and OPT-IML-175B). These findings highlight Cappy’s capabilities and parameter efficiency, which can be credited to its scoring-based pre-training strategy that integrates contrastive information by differentiating between high-quality and low-quality responses. On the contrary, previous multi-task LLMs depend exclusively on teacher-forcing training that utilizes only the ground truth responses.\n\n\n\n\n\n\n\nThe overall accuracy averaged over eleven test tasks from PromptSource. “RM” refers to a pre-trained RLHF reward model. Cappy matches the best ones among existing multi-task LLMs.\n\nBIG-Bench, a set of manually curated tasks that are considered beyond the capability of many LLMs. We focus on all the 45 generation BIG-Bench tasks, specifically those that do not offer pre-established answer choices. We evaluate the performance using the Rouge-L score (representing the overall similarity between model generations and corresponding ground truths) on every test set, reporting the average score across 45 tests. In this experiment, all variants of FLAN-T5 serve as the backbone LLMs, and the foundational FLAN-T5 models are frozen. These results, shown below, suggest that Cappy enhances the performance of FLAN-T5 models by a large margin, consistently outperforming the most effective baseline achieved through sample selection using self-scoring of the LLM itself.\n\n\n\n\nThe averaged Rouge-L score over 45 complex tasks within BIG-Bench. The x-axis refers to FLAN-T5 models of different sizes. Every dashed line represents an approach working on FLAN-T5s. Self-scoring refers to using the cross-entropy of LLM to select responses. Cappy enhances the performance of FLAN-T5 models by a large margin.\n\nConclusion\nAcknowledgments\nThanks to Bowen Tan, Jindong Chen, Lei Meng, Abhanshu Sharma and Ewa Dominowska for their valuable feedback. We would also like to thank Eric Xing and Zhiting Hu for their suggestions."
  },
  {
    "source": "Google AI Blog",
    "title": "Talk like a graph: Encoding graphs for large language models",
    "link": "http://blog.research.google/2024/03/talk-like-graph-encoding-graphs-for.html",
    "pubDate": "2024-03-12T21:15:00.000Z",
    "contentSnippet": "Posted by Bahare Fatemi and Bryan Perozzi, Research Scientists, Google Research\n\n\n\n\nImagine all the things around you — your friends, tools in your kitchen, or even the parts of your bike. They are all connected in different ways. In computer science, the term graph is used to describe connections between objects. Graphs consist of nodes (the objects themselves) and edges (connections between two nodes, indicating a  relationship between them). Graphs are everywhere now. The internet itself is a giant graph of websites linked together. Even the knowledge search engines use is organized in a graph-like way.\n\n\n\n\n\nFurthermore, consider the remarkable advancements in artificial intelligence — such as chatbots that can write stories in seconds, and even software that can interpret medical reports. This exciting progress is largely thanks to large language models (LLMs). New LLM technology is constantly being developed for different uses. \n\n\nSince graphs are everywhere and LLM technology is on the rise, in “Talk like a Graph: Encoding Graphs for Large Language Models”, presented at ICLR 2024, we present a way to teach powerful LLMs how to better reason with graph information. Graphs are a useful way to organize information, but LLMs are mostly trained on regular text. The objective is to test different techniques to see what works best and gain practical insights. Translating graphs into text that LLMs can understand is a remarkably complex task. The difficulty stems from the inherent complexity of graph structures with multiple nodes and the intricate web of edges that connect them. Our work studies how to take a graph and translate it into a format that an LLM can understand. We also design a benchmark called GraphQA to study different approaches on different graph reasoning problems and show how to phrase a graph-related problem in a way that enables the LLM to solve the graph problem. We show that LLM performance on graph reasoning tasks varies on three fundamental levels: 1) the graph encoding method, 2) the nature of the graph task itself, and 3) interestingly, the very structure of the graph considered. These findings give us clues on how to best represent graphs for LLMs. Picking the right method can make the LLM up to 60% better at graph tasks!\n\n\n\n\n\n\n\nPictured, the process of encoding a graph as text using two different approaches and feeding the text and a question about the graph to the LLM.\n\nGraphs as text\nGraphQA. Think of GraphQA as an exam designed to evaluate powerful LLMs on graph-specific problems. We want to see how well LLMs can understand and solve problems that involve graphs in different setups. To create a comprehensive and realistic exam for LLMs, we don’t just use one type of graph, we use a mix of graphs ensuring breadth in the number of connections. This is mainly because different graph types make solving such problems easier or harder. This way, GraphQA can help expose biases in how an LLM thinks about the graphs, and the whole exam gets closer to a realistic setup that LLMs might encounter in the real world.\n\n\n\n\n\n\n\nOverview of our framework for reasoning with graphs using LLMs.\n\nErdős-Rényi, scale-free networks, Barabasi-Albert model, and stochastic block model, as well as simpler graph structures like paths, complete graphs, and star graphs, providing a diverse set of data for training.\n\n\nWhen working with graphs, we also need to find ways to ask graph-related questions that LLMs can understand.  Prompting heuristics are different strategies for doing this. Let's break down the common ones:\n\n\n\nZero-shot: simply describe the task (\"Is there a cycle in this graph?\") and tell the LLM to go for it. No examples provided.\n\n\nFew-shot: This is like giving the LLM a mini practice test before the real deal. We provide a few example graph questions and their correct answers.\n\n\nChain-of-Thought: Here, we show the LLM how to break down a problem step-by-step with examples. The goal is to teach it to generate its own \"thought process\" when faced with new graphs.\n\n\nZero-CoT: Similar to CoT, but instead of training examples, we give the LLM a simple prompt, like \"Let's think step-by-step,\" to trigger its own problem-solving breakdown.\n\n\nBAG (build a graph): This is specifically for graph tasks. We add the phrase \"Let's build a graph...\" to the description, helping the LLM focus on the graph structure.\n\n\n\nWe explored different ways to translate graphs into text that LLMs can work with. Our key questions were:\n\n\n\nNode encoding: How do we represent individual nodes? Options tested include simple integers, common names (people, characters), and letters.\n\n\nEdge encoding: How do we describe the relationships between nodes? Methods involved parenthesis notation, phrases like \"are friends\", and symbolic representations like arrows.\n\n\n\nVarious node and edge encodings were combined systematically. This led to functions like the ones in the following figure:\n\n\n\n\nExamples of graph encoding functions used to encode graphs via text.\n\nAnalysis and results\nHow LLMs handle graph tasks \nLLMs struggle: On most of these basic tasks, LLMs did not do much better than a random guess. \n\n\nEncoding matters significantly: How we represent the graph as text has a great effect on LLM performance. The \"incident\" encoding excelled for most of the tasks in general.\n\n\n\nOur results are summarized in the following chart. \n\n\n\n\n\n\nComparison of various graph encoder functions based on their accuracy on different graph tasks. The main conclusion from this figure is that the graph encoding functions matter significantly.\n\nBigger is (usually) better \nPaLM 2. Here is a summary of our findings:\n\n\n\nIn general, bigger models did better on graph reasoning tasks. It seems like the extra parameters gave them space to learn more complex patterns.\n\n\nOddly, size didn't matter as much for the “edge existence” task (finding out if two nodes in a graph are connected).\n\n\nEven the biggest LLM couldn't consistently beat a simple baseline solution on the cycle check problem (finding out if a graph contains a cycle or not). This shows LLMs still have room to improve with certain graph tasks.\n\n\n\n\n\n\n\n\n\nEffect of model capacity on graph reasoning task for PaLM 2-XXS, XS, S, and L.\n\nDo different graph shapes confuse LLMs \n\n\nSamples of graphs generated with different graph generators from GraphQA. ER, BA, SBM, and SFN refers to Erdős–Rényi, Barabási–Albert, Stochastic Block Model, and Scale-Free Network respectively.\n\n\n\nComparing different graph generators on different graph tasks. The main observation here is that graph structure has a significant impact on the LLM’s performance. ER, BA, SBM, and SFN refers to Erdős–Rényi, Barabási–Albert, Stochastic Block Model, and Scale-Free Network respectively.\n\nConclusion\nHow to translate the graph to text: how we represent the graph as text significantly influences LLM performance. The incident encoding excelled for most of the tasks in general..\n\n\nTask type: Certain types of graph questions tend to be harder for LLMs, even with a good translation from graph to text.\n\n\nGraph structure: Surprisingly, the \"shape\" of the graph that on which we do inference (dense with connections, sparse, etc.) influences how well an LLM does.\n\n\n\nThis study revealed key insights about how to prepare graphs for LLMs. The right encoding techniques can significantly boost an LLM's accuracy on graph problems (ranging from around 5% to over 60% improvement). Our new benchmark, GraphQA, will help drive further research in this area.\n\n\n\n\n\n    \nAcknowledgements\nWe would like to express our gratitude to our co-author, Jonathan Halcrow, for his valuable contributions to this work. We express our sincere gratitude to Anton Tsitsulin, Dustin Zelle, Silvio Lattanzi, Vahab Mirrokni, and the entire graph mining team at Google Research, for their insightful comments, thorough proofreading, and constructive feedback which greatly enhanced the quality of our work. We would also like to extend special thanks to Tom Small for creating the animation used in this post."
  }
]